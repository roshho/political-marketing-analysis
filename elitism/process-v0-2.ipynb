{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1) Download webpage (HTML only)\n",
    "2) Rename HTML file to ``input.htm``\n",
    "3) Insert your ChatGPT API key\n",
    "4) Run \"home page praser\" in the following, will output \n",
    "    - Method: Due to variations in page structures, it is easier to chunk and parse through HTML file into CSV file\n",
    "    - In: ``input.htm``. Out: ``1homepage.csv``\n",
    "5) Run ``obtain about page``\n",
    "    - In: ``1homepage.csv``. Out: ``2aboutpage.csv``\n",
    "6) Manually clean up formatting and potential artifacts\n",
    "    - This can be due to ad-hoc instances where canidates is still being enstated or remove thus no URL sor artifact from chunking the HTML file for processing\n",
    "7) Run ``obtain wikipedia link via Google``\n",
    "    - In: ``2aboutpage.csv``. Out: ``3aboutpage-n-wiki.csv``\n",
    "8) Run ``wiki vcard scraper`` to see where they went to university via wikipedia\n",
    "    - In: ``3aboutpage-n-wiki.csv``. Out: ``4aboutpage-n-wiki-vcard.csv``\n",
    "    - Common issues: If incorrect ans is returned, likely due to wrong wikipedia link or couldn't find exact potlicain name. University on wikipedia vcard can also be wrapped weirdly in CSS, thus may need to manually eneter university name\n",
    "9) Run ``Clean wikipedia vcard university into individual columns``\n",
    "    - In: ``4aboutpage-n-wiki-vcard.csv``. Out: ``5aboutpage-n-cleaned-uni.csv``\n",
    "10) Manully clean up formatting and artifacts under \"office-start-date\" column. \n",
    "    - Anomolies, formatting issues or x2 dates (instead of x1 date) is likely the wrong wikipedia page found.\n",
    "10) Run ``scrape about page bio``\n",
    "    - In: ``5aboutpage-n-cleaned-uni.csv``. Out: ``6scraped-bio-n-cleaned-uni.csv``\n",
    "11) Run ``check if uni is prestige & if bio mentioned``\n",
    "    - In: ``6scraped-bio-n-cleaned-uni.csv``. Out: ``7bio-checked-n-uni-checked.csv``\n",
    "    - This also requires ``T25-list.csv``, listing top 25 universities in general and liberal arts category from US News. Alternate names and syntax of unis are also added here, e.g. \"University of California, Berkeley\", \"University of California - Berkeley\", \"Cal\", \"UCB\", \"Berkeley\".\n",
    "    - This will check if university is elite & extract the sentences that mentioned \"academy\", \"university\", \"college\", \"school\" in ``uni_in_bio`` column. This is to aid manual vetting and fuzzy logic matching after. \n",
    "12) Run ``check for university aliases``\n",
    "    - In: ``7bio-checked-n-uni-checked.csv``. Out: ``8final_university_matching.csv``\n",
    "    - This uses fuzzymatching to check if university is mentioned.\n",
    "13) Run ``LLM to vet uni mention in bio``\n",
    "    - In:``8final_university_matching.csv``. Out: ``9LLM-vetted.csv``\n",
    "    - Using deepseek + fuzzy matching to check if uni has been mentioned. \n",
    "14) Check \"LLM_vetted_result\" for discrepencies. 1 is discrepency and may require further manual revision by looking at ``uni_in_bio`` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m venv bio-scrape\n",
    "! .\\bio-scrape\\Scripts\\activate\n",
    "! pip install jupyter\n",
    "! python -m ipykernel install --user --name=bio-scrape --display-name \"Python (bio-scrape)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.2-py3-none-any.whl.metadata (64 kB)\n",
      "     ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "     ------------------------- -------------- 41.0/64.7 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 64.7/64.7 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from openai) (4.13.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 41.0/599.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 41.0/599.1 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 174.1/599.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 358.4/599.1 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 599.1/599.1 kB 2.7 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Downloading pydantic-2.11.2-py3-none-any.whl (443 kB)\n",
      "   ---------------------------------------- 0.0/443.3 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 368.6/443.3 kB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/443.3 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 443.3/443.3 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/2.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/2.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.6 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.9.0 openai-1.70.0 pydantic-2.11.2 pydantic-core-2.33.1 tqdm-4.67.1 typing-inspection-0.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas requests openai beautifulsoup4 aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd                   \n",
    "import requests                       \n",
    "import json                           \n",
    "import re\n",
    "import openai                             \n",
    "import asyncio                        \n",
    "from bs4 import BeautifulSoup       \n",
    "import csv\n",
    "import os \n",
    "import time\n",
    "import urllib.parse\n",
    "from urllib.parse import urljoin\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: [Errno 2] No such file or directory: 'input.htm'\n"
     ]
    }
   ],
   "source": [
    "def call_chatgpt(prompt: str) -> str:\n",
    "    \"\"\"Call OpenAI's ChatGPT API to extract data from HTML.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts data from HTML tables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=4096,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_html(content: str, chunk_size: int = 10000):\n",
    "    \"\"\"Split HTML content into manageable chunks.\"\"\"\n",
    "    return [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "\n",
    "def extract_data_from_html(html_chunks):\n",
    "    \"\"\"Extract structured data from HTML chunks.\"\"\"\n",
    "    data = []\n",
    "    for chunk in html_chunks:\n",
    "        prompt = f\"Extract a list of politicians from the following HTML chunk, including their state, district, name, party, and URL, into a CSV. \\n\\n{chunk}\"\n",
    "        result = call_chatgpt(prompt)\n",
    "        if result:\n",
    "            rows = result.strip().split(\"\\n\")\n",
    "            for row in rows:\n",
    "                if row:\n",
    "                    data.append(row.split(\",\"))\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    \"\"\"Save extracted data to a CSV file.\"\"\"\n",
    "    headers = [\"State\", \"District\", \"Name\", \"URL\", \"Party\"]\n",
    "    try:\n",
    "        with open(output_file, mode='w', newline='', encoding='UTF-16') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(headers)\n",
    "            writer.writerows(data)\n",
    "        print(f\"Data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {e}\")\n",
    "\n",
    "def main(html_file: str, output_file: str):\n",
    "    \"\"\"Main function to process an HTML file and save extracted data.\"\"\"\n",
    "    try:\n",
    "        with open(html_file, 'r', encoding='UTF-16') as file:\n",
    "            content = file.read()\n",
    "        html_chunks = split_html(content)\n",
    "        extracted_data = extract_data_from_html(html_chunks)\n",
    "        save_to_csv(extracted_data, output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('input.htm', '1homepage.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain about page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: https://barrymoore.house.gov -> https://barrymoore.house.gov/about\n",
      "Processed: https://figures.house.gov/ -> https://figures.house.gov/about\n",
      "Processed: https://mikerogers.house.gov/ -> https://mikerogers.house.gov/about/\n",
      "Processed: https://aderholt.house.gov/ -> https://aderholt.house.gov/about-robert\n",
      "Processed: https://strong.house.gov -> https://strong.house.gov/about\n",
      "Processed: https://palmer.house.gov/ -> https://palmer.house.gov/about/full-biography\n",
      "Processed: https://sewell.house.gov/ -> https://sewell.house.gov/meet-terri\n",
      "Processed: https://begich.house.gov/ -> https://begich.house.gov/about\n",
      "Processed: https://radewagen.house.gov -> https://radewagen.house.gov/about\n",
      "Processed: https://schweikert.house.gov/ -> https://schweikert.house.gov/about/\n",
      "Processed: https://crane.house.gov -> https://crane.house.gov/about/\n",
      "Processed: https://ansari.house.gov/ -> https://ansari.house.gov/about\n",
      "Processed: https://stanton.house.gov/ -> https://stanton.house.gov/about\n",
      "Processed: https://biggs.house.gov -> https://biggs.house.gov/about\n",
      "Processed: https://ciscomani.house.gov -> https://www.facebook.com/profile.php?id=100089186535085\n",
      "Error fetching http://: Invalid URL 'http://': No host supplied\n",
      "Processed: http:// -> Not Found\n",
      "Processed: https://hamadeh.house.gov/ -> https://hamadeh.house.gov/about\n",
      "Processed: https://gosar.house.gov/ -> https://gosar.house.gov/biography/\n",
      "Processed: https://crawford.house.gov/ -> Not Found\n",
      "Processed: https://hill.house.gov/ -> https://hill.house.gov/biography\n",
      "Processed: https://womack.house.gov/ -> https://womack.house.gov/biography\n",
      "Processed: https://westerman.house.gov/ -> https://westerman.house.gov/about\n",
      "Processed: https://lamalfa.house.gov -> https://lamalfa.house.gov/about\n",
      "Processed: https://huffman.house.gov -> https://huffman.house.gov/meet-jared\n",
      "Processed: https://kiley.house.gov -> https://www.facebook.com/profile.php?id=100089382125341\n",
      "Processed: https://mikethompson.house.gov/ -> https://mikethompson.house.gov/about-mike\n",
      "Processed: https://mcclintock.house.gov/ -> https://mcclintock.house.gov/about/votes-and-legislation\n",
      "Processed: https://bera.house.gov -> https://bera.house.gov/about/\n",
      "Processed: https://matsui.house.gov -> https://matsui.house.gov/biography\n",
      "Processed: https://garamendi.house.gov/ -> https://garamendi.house.gov/about\n",
      "Processed: https://harder.house.gov/ -> https://harder.house.gov/about\n",
      "Processed: https://desaulnier.house.gov/ -> https://desaulnier.house.gov/about\n",
      "Processed: https://pelosi.house.gov/ -> https://pelosi.house.gov/biography\n",
      "Processed: https://simon.house.gov/ -> https://simon.house.gov/about\n",
      "Processed: https://gray.house.gov/ -> https://gray.house.gov/about\n",
      "Processed: https://swalwell.house.gov -> https://swalwell.house.gov/about/full-biography\n",
      "Processed: https://kevinmullin.house.gov -> https://kevinmullin.house.gov/about/\n",
      "Processed: https://liccardo.house.gov/ -> https://liccardo.house.gov/about\n",
      "Processed: https://khanna.house.gov -> https://khanna.house.gov/about\n",
      "Processed: https://lofgren.house.gov/ -> https://lofgren.house.gov/about\n",
      "Processed: https://panetta.house.gov -> https://panetta.house.gov/about\n",
      "Processed: https://fong.house.gov/ -> https://fong.house.gov/about\n",
      "Processed: https://costa.house.gov/ -> https://costa.house.gov/about/full-biography\n",
      "Processed: https://valadao.house.gov -> https://valadao.house.gov/about/\n",
      "Processed: https://obernolte.house.gov -> https://obernolte.house.gov/about\n",
      "Processed: https://carbajal.house.gov -> https://carbajal.house.gov/biography\n",
      "Processed: https://ruiz.house.gov -> https://ruiz.house.gov/about\n",
      "Processed: https://juliabrownley.house.gov -> https://bsky.app/profile/juliabrownley.house.gov\n",
      "Processed: https://whitesides.house.gov/ -> https://whitesides.house.gov/about\n",
      "Processed: https://chu.house.gov/ -> https://chu.house.gov/about\n",
      "Processed: https://rivas.house.gov/ -> https://rivas.house.gov/about\n",
      "Processed: https://friedman.house.gov/ -> https://friedman.house.gov/about\n",
      "Processed: https://cisneros.house.gov/ -> https://cisneros.house.gov/about\n",
      "Processed: https://sherman.house.gov -> https://sherman.house.gov/about/about-brad-sherman\n",
      "Processed: https://aguilar.house.gov/ -> https://aguilar.house.gov/about/\n",
      "Processed: https://gomez.house.gov/ -> https://gomez.house.gov/biography\n",
      "Processed: https://torres.house.gov/ -> https://torres.house.gov/about\n",
      "Processed: https://lieu.house.gov/ -> https://lieu.house.gov/about\n",
      "Processed: https://kamlager-dove.house.gov -> https://kamlager-dove.house.gov/about\n",
      "Processed: https://lindasanchez.house.gov/ -> https://lindasanchez.house.gov/about\n",
      "Processed: https://takano.house.gov -> https://takano.house.gov/about\n",
      "Processed: https://youngkim.house.gov -> https://youngkim.house.gov/about/\n",
      "Processed: https://calvert.house.gov/ -> https://calvert.house.gov/about-ken\n",
      "Processed: https://robertgarcia.house.gov -> https://robertgarcia.house.gov/about\n",
      "Processed: https://waters.house.gov -> https://waters.house.gov/about-maxine\n",
      "Processed: https://barragan.house.gov -> https://barragan.house.gov/about/\n",
      "Processed: https://tran.house.gov/ -> https://tran.house.gov/about\n",
      "Processed: https://correa.house.gov -> https://correa.house.gov/about/biography\n",
      "Processed: https://min.house.gov/ -> https://min.house.gov/about\n",
      "Processed: https://issa.house.gov -> https://issa.house.gov/about\n",
      "Processed: https://levin.house.gov/ -> https://levin.house.gov/about\n",
      "Processed: https://scottpeters.house.gov -> https://scottpeters.house.gov/about\n",
      "Processed: https://sarajacobs.house.gov/ -> https://sarajacobs.house.gov/about\n",
      "Processed: https://vargas.house.gov/ -> https://vargas.house.gov/about\n",
      "Processed: https://degette.house.gov -> https://degette.house.gov/biography\n",
      "Processed: https://neguse.house.gov/ -> https://neguse.house.gov/about\n",
      "Processed: https://hurd.house.gov/ -> https://hurd.house.gov/about\n",
      "Processed: https://boebert.house.gov -> https://boebert.house.gov/about/biography\n",
      "Processed: https://crank.house.gov/ -> https://crank.house.gov/about\n",
      "Processed: https://crow.house.gov/ -> https://crow.house.gov/about\n",
      "Processed: https://pettersen.house.gov -> https://pettersen.house.gov/about/\n",
      "Processed: https://gabeevans.house.gov/ -> https://gabeevans.house.gov/about\n",
      "Processed: https://larson.house.gov/ -> https://larson.house.gov/about/meet-john\n",
      "Processed: https://courtney.house.gov/ -> https://courtney.house.gov/about\n",
      "Processed: https://delauro.house.gov/ -> https://delauro.house.gov/about-rosa\n",
      "Processed: https://himes.house.gov/ -> https://himes.house.gov/about-jim\n",
      "Processed: https://hayes.house.gov -> https://hayes.house.gov/about\n",
      "Processed: https://mcbride.house.gov/ -> https://mcbride.house.gov/about\n",
      "Processed: https://norton.house.gov/ -> https://norton.house.gov/contact/request-an-appearance\n",
      "Processed: https://clerk.house.gov/members/FL01/vacancy -> https://clerk.house.gov/members\n",
      "Processed: https://dunn.house.gov -> https://dunn.house.gov/about\n",
      "Processed: https://cammack.house.gov -> https://cammack.house.gov/about\n",
      "Processed: https://bean.house.gov -> https://bean.house.gov/about\n",
      "Processed: https://rutherford.house.gov -> https://rutherford.house.gov/about\n",
      "Processed: https://clerk.house.gov/members/FL06/vacancy -> https://clerk.house.gov/members\n",
      "Processed: https://mills.house.gov -> https://mills.house.gov/about\n",
      "Processed: https://haridopolos.house.gov/ -> https://haridopolos.house.gov/about\n",
      "Processed: https://soto.house.gov -> https://soto.house.gov/about\n",
      "Processed: https://frost.house.gov -> https://frost.house.gov/about\n",
      "Processed: https://webster.house.gov/ -> https://webster.house.gov/about-dan\n",
      "Processed: https://bilirakis.house.gov/ -> https://bilirakis.house.gov/about\n",
      "Processed: https://luna.house.gov -> Not Found\n",
      "Processed: https://castor.house.gov/ -> https://bsky.app/profile/usrepkathycastor.bsky.social\n",
      "Processed: https://laurellee.house.gov -> https://laurellee.house.gov/about\n",
      "Processed: https://buchanan.house.gov/ -> https://buchanan.house.gov/about-vern\n",
      "Processed: https://steube.house.gov/ -> https://steube.house.gov/meet-greg/\n",
      "Processed: https://franklin.house.gov -> https://franklin.house.gov/about/\n",
      "Processed: https://donalds.house.gov -> https://donalds.house.gov/about/\n",
      "Processed: https://cherfilus-mccormick.house.gov/ -> https://cherfilus-mccormick.house.gov/about\n",
      "Processed: https://mast.house.gov -> https://mast.house.gov/about\n",
      "Processed: https://frankel.house.gov -> https://bsky.app/profile/frankel.house.gov\n",
      "Processed: https://moskowitz.house.gov -> https://bsky.app/profile/repmoskowitz.bsky.social\n",
      "Processed: https://wilson.house.gov/ -> https://wilson.house.gov/about\n",
      "Processed: https://wassermanschultz.house.gov/ -> https://wassermanschultz.house.gov/biography/\n",
      "Processed: https://mariodiazbalart.house.gov/ -> https://mariodiazbalart.house.gov/about\n",
      "Processed: https://salazar.house.gov -> https://salazar.house.gov/about\n",
      "Processed: https://gimenez.house.gov -> https://gimenez.house.gov/about\n",
      "Processed: https://buddycarter.house.gov/ -> https://buddycarter.house.gov/about/\n",
      "Processed: https://bishop.house.gov/ -> https://bishop.house.gov/about\n",
      "Processed: https://jack.house.gov/ -> https://jack.house.gov/about\n",
      "Processed: https://hankjohnson.house.gov/ -> https://hankjohnson.house.gov/about\n",
      "Processed: https://nikemawilliams.house.gov -> Not Found\n",
      "Processed: https://mcbath.house.gov -> https://mcbath.house.gov/about\n",
      "Processed: https://mccormick.house.gov -> https://mccormick.house.gov/about\n",
      "Processed: https://austinscott.house.gov/ -> https://austinscott.house.gov/meet-austin\n",
      "Processed: https://clyde.house.gov -> https://clyde.house.gov/about\n",
      "Processed: https://collins.house.gov -> https://collins.house.gov/about\n",
      "Processed: https://loudermilk.house.gov -> https://loudermilk.house.gov/biography/\n",
      "Processed: https://allen.house.gov -> https://allen.house.gov/about\n",
      "Processed: https://davidscott.house.gov/ -> https://davidscott.house.gov/biography/\n",
      "Processed: https://greene.house.gov -> https://greene.house.gov/about\n",
      "Processed: https://moylan.house.gov -> https://moylan.house.gov/about\n",
      "Processed: https://case.house.gov/ -> https://case.house.gov/about\n",
      "Processed: https://tokuda.house.gov -> https://tokuda.house.gov/about/bio\n",
      "Processed: https://fulcher.house.gov/ -> https://fulcher.house.gov/about\n",
      "Processed: https://simpson.house.gov -> https://simpson.house.gov/biography/\n",
      "Processed: https://jonathanjackson.house.gov -> https://jonathanjackson.house.gov/about\n",
      "Processed: https://robinkelly.house.gov/ -> https://robinkelly.house.gov/about\n",
      "Processed: https://ramirez.house.gov -> https://ramirez.house.gov/about\n",
      "Processed: https://chuygarcia.house.gov/ -> https://chuygarcia.house.gov/about\n",
      "Processed: https://quigley.house.gov/ -> https://quigley.house.gov/about\n",
      "Processed: https://casten.house.gov -> https://casten.house.gov/about\n",
      "Processed: https://davis.house.gov -> https://davis.house.gov/about\n",
      "Processed: https://krishnamoorthi.house.gov -> https://krishnamoorthi.house.gov/about\n",
      "Processed: https://schakowsky.house.gov -> https://schakowsky.house.gov/contact/request-an-appearance\n",
      "Processed: https://schneider.house.gov -> https://schneider.house.gov/about\n",
      "Processed: https://foster.house.gov -> https://foster.house.gov/about\n",
      "Processed: https://bost.house.gov/ -> https://bost.house.gov/about\n",
      "Processed: https://budzinski.house.gov -> https://budzinski.house.gov/posts/budzinski-duckworth-meet-with-local-veterans-and-metro-east-vso-leaders-to-discuss-trump-administrations-attacks-on-the-veteran-community\n",
      "Processed: https://underwood.house.gov/ -> https://underwood.house.gov/about\n",
      "Processed: https://marymiller.house.gov -> https://marymiller.house.gov/about\n",
      "Processed: https://lahood.house.gov/ -> https://lahood.house.gov/about\n",
      "Processed: https://sorensen.house.gov -> https://sorensen.house.gov/about-eric\n",
      "Processed: https://mrvan.house.gov -> https://mrvan.house.gov/about\n",
      "Processed: https://yakym.house.gov -> Not Found\n",
      "Processed: https://stutzman.house.gov/ -> https://stutzman.house.gov/about\n",
      "Processed: https://baird.house.gov/ -> https://baird.house.gov/about/\n",
      "Processed: https://spartz.house.gov -> https://spartz.house.gov/about\n",
      "Processed: https://shreve.house.gov/ -> https://shreve.house.gov/about\n",
      "Processed: https://carson.house.gov/ -> https://carson.house.gov/about/our-district\n",
      "Processed: https://messmer.house.gov/ -> https://messmer.house.gov/about/\n",
      "Processed: https://houchin.house.gov -> https://houchin.house.gov/about\n",
      "Processed: https://millermeeks.house.gov/ -> https://millermeeks.house.gov/about\n",
      "Processed: https://hinson.house.gov -> https://hinson.house.gov/about\n",
      "Processed: https://nunn.house.gov -> https://nunn.house.gov/about/\n",
      "Processed: https://feenstra.house.gov -> https://feenstra.house.gov/about\n",
      "Processed: https://mann.house.gov -> https://mann.house.gov/about\n",
      "Processed: https://schmidt.house.gov/ -> https://schmidt.house.gov/about\n",
      "Processed: https://davids.house.gov/ -> https://davids.house.gov/about\n",
      "Processed: https://estes.house.gov/ -> https://estes.house.gov/biography\n",
      "Processed: https://comer.house.gov/ -> https://comer.house.gov/about\n",
      "Processed: https://guthrie.house.gov/ -> https://guthrie.house.gov/about/\n",
      "Processed: https://mcgarvey.house.gov -> https://mcgarvey.house.gov/about\n",
      "Processed: https://massie.house.gov -> https://massie.house.gov/about\n",
      "Processed: https://halrogers.house.gov/ -> https://halrogers.house.gov/about-hal\n",
      "Processed: https://barr.house.gov/ -> https://barr.house.gov/about-andy-bio\n",
      "Processed: https://scalise.house.gov/ -> https://scalise.house.gov/about/biography\n",
      "Processed: https://troycarter.house.gov -> https://troycarter.house.gov/about\n",
      "Processed: https://clayhiggins.house.gov -> https://clayhiggins.house.gov/about/\n",
      "Processed: https://mikejohnson.house.gov -> https://mikejohnson.house.gov/about\n",
      "Processed: https://letlow.house.gov -> https://letlow.house.gov/socialsecurity\n",
      "Processed: https://fields.house.gov/ -> https://fields.house.gov/about\n",
      "Processed: https://pingree.house.gov/ -> https://pingree.house.gov/about/\n",
      "Processed: https://golden.house.gov -> https://golden.house.gov/about\n",
      "Processed: https://harris.house.gov/ -> https://harris.house.gov/about\n",
      "Processed: https://olszewski.house.gov/ -> https://olszewski.house.gov/about\n",
      "Processed: https://elfreth.house.gov/ -> https://elfreth.house.gov/about\n",
      "Processed: https://ivey.house.gov -> https://ivey.house.gov/about\n",
      "Processed: https://hoyer.house.gov/ -> https://hoyer.house.gov/about-steny\n",
      "Processed: https://mcclaindelaney.house.gov/ -> https://mcclaindelaney.house.gov/about\n",
      "Processed: https://mfume.house.gov/ -> https://mfume.house.gov/about\n",
      "Processed: https://raskin.house.gov -> https://raskin.house.gov/about\n",
      "Processed: https://neal.house.gov/ -> https://neal.house.gov/about/\n",
      "Processed: https://mcgovern.house.gov/ -> https://mcgovern.house.gov/about\n",
      "Processed: https://trahan.house.gov -> https://bsky.app/profile/trahan.house.gov\n",
      "Processed: https://auchincloss.house.gov -> https://auchincloss.house.gov/about\n",
      "Processed: https://katherineclark.house.gov/index.cfm/home -> https://katherineclark.house.gov/meet-katherine\n",
      "Processed: https://moulton.house.gov/ -> https://moulton.house.gov/about\n",
      "Processed: https://pressley.house.gov -> https://pressley.house.gov/about/\n",
      "Processed: https://lynch.house.gov/ -> https://lynch.house.gov/about\n",
      "Processed: https://keating.house.gov/ -> https://keating.house.gov/about-bill\n",
      "Processed: https://bergman.house.gov -> https://bergman.house.gov/biography\n",
      "Processed: https://moolenaar.house.gov/ -> https://moolenaar.house.gov/about\n",
      "Processed: https://scholten.house.gov -> https://scholten.house.gov/about\n",
      "Processed: https://huizenga.house.gov/ -> https://huizenga.house.gov/meet-bill/\n",
      "Processed: https://walberg.house.gov/ -> https://walberg.house.gov/contact/request-meeting\n",
      "Processed: https://debbiedingell.house.gov/ -> https://debbiedingell.house.gov/about/\n",
      "Processed: https://barrett.house.gov/ -> https://barrett.house.gov/about\n",
      "Processed: https://mcdonaldrivet.house.gov/ -> https://mcdonaldrivet.house.gov/node/21\n",
      "Processed: https://mcclain.house.gov -> https://mcclain.house.gov/about\n",
      "Processed: https://james.house.gov -> https://james.house.gov/about/\n",
      "Processed: https://stevens.house.gov/ -> https://stevens.house.gov/about\n",
      "Processed: https://tlaib.house.gov/ -> Not Found\n",
      "Processed: https://thanedar.house.gov -> https://thanedar.house.gov/about\n",
      "Processed: https://finstad.house.gov/ -> https://finstad.house.gov/about\n",
      "Processed: https://craig.house.gov -> https://craig.house.gov/about\n",
      "Processed: https://morrison.house.gov/ -> https://morrison.house.gov/about\n",
      "Processed: https://mccollum.house.gov -> https://mccollum.house.gov/about\n",
      "Processed: https://omar.house.gov/ -> https://omar.house.gov/about\n",
      "Processed: https://emmer.house.gov/ -> https://emmer.house.gov/about\n",
      "Processed: https://fischbach.house.gov -> https://fischbach.house.gov/about\n",
      "Processed: https://stauber.house.gov -> https://stauber.house.gov/about\n",
      "Processed: https://trentkelly.house.gov/ -> https://trentkelly.house.gov/biography\n",
      "Processed: https://benniethompson.house.gov/ -> https://benniethompson.house.gov/about\n",
      "Processed: https://guest.house.gov -> https://guest.house.gov/about\n",
      "Processed: https://ezell.house.gov -> https://ezell.house.gov/about/\n",
      "Processed: https://bell.house.gov/ -> https://bell.house.gov/about\n",
      "Processed: https://wagner.house.gov -> https://wagner.house.gov/about\n",
      "Processed: https://onder.house.gov/ -> https://onder.house.gov/about\n",
      "Processed: https://alford.house.gov -> https://facebook.com/profile.php?id=100089595940547/\n",
      "Processed: https://cleaver.house.gov -> https://cleaver.house.gov/about\n",
      "Processed: https://graves.house.gov/ -> https://graves.house.gov/about\n",
      "Processed: https://burlison.house.gov -> https://burlison.house.gov/about\n",
      "Processed: https://jasonsmith.house.gov -> https://jasonsmith.house.gov/meet-jason/\n",
      "Processed: https://zinke.house.gov -> https://zinke.house.gov/about\n",
      "Processed: https://downing.house.gov/ -> https://downing.house.gov/live\n",
      "Processed: https://flood.house.gov/ -> https://flood.house.gov/about\n",
      "Processed: https://bacon.house.gov -> https://bacon.house.gov/about/about-don.htm\n",
      "Processed: https://adriansmith.house.gov/ -> https://adriansmith.house.gov/about\n",
      "Processed: https://titus.house.gov/ -> https://titus.house.gov/about/about-dina-titus.htm\n",
      "Processed: https://amodei.house.gov -> https://amodei.house.gov/meet-mark\n",
      "Processed: https://susielee.house.gov -> https://susielee.house.gov/official-biography\n",
      "Processed: https://horsford.house.gov -> https://horsford.house.gov/about/about-steven\n",
      "Processed: https://pappas.house.gov -> https://pappas.house.gov/about\n",
      "Processed: https://goodlander.house.gov/ -> https://goodlander.house.gov/about\n",
      "Processed: https://norcross.house.gov -> https://norcross.house.gov/about\n",
      "Processed: https://vandrew.house.gov -> https://vandrew.house.gov/about\n",
      "Processed: https://conaway.house.gov/ -> https://conaway.house.gov/about\n",
      "Processed: https://chrissmith.house.gov/ -> https://chrissmith.house.gov/biography\n",
      "Processed: https://gottheimer.house.gov -> Not Found\n",
      "Processed: https://pallone.house.gov -> https://pallone.house.gov/about\n",
      "Processed: https://kean.house.gov -> https://kean.house.gov/about\n",
      "Processed: https://menendez.house.gov -> https://menendez.house.gov/about\n",
      "Processed: https://pou.house.gov/ -> https://pou.house.gov/about\n",
      "Processed: https://mciver.house.gov/ -> https://mciver.house.gov/about\n",
      "Processed: https://sherrill.house.gov/ -> https://sherrill.house.gov/about\n",
      "Processed: https://watsoncoleman.house.gov/ -> https://watsoncoleman.house.gov/about\n",
      "Processed: https://stansbury.house.gov/ -> https://stansbury.house.gov/about\n",
      "Processed: https://vasquez.house.gov -> https://vasquez.house.gov/about\n",
      "Processed: https://fernandez.house.gov -> https://fernandez.house.gov/about\n",
      "Processed: https://lalota.house.gov -> https://lalota.house.gov/about\n",
      "Processed: https://garbarino.house.gov -> https://garbarino.house.gov/about\n",
      "Processed: https://suozzi.house.gov -> https://suozzi.house.gov/about\n",
      "Processed: https://gillen.house.gov/ -> https://gillen.house.gov/about\n",
      "Processed: https://meeks.house.gov -> https://meeks.house.gov/about/biography\n",
      "Processed: https://meng.house.gov -> https://meng.house.gov/about\n",
      "Processed: https://velazquez.house.gov -> https://velazquez.house.gov/about\n",
      "Processed: https://jeffries.house.gov -> https://jeffries.house.gov/about/\n",
      "Processed: https://clarke.house.gov/ -> https://clarke.house.gov/about/\n",
      "Processed: https://goldman.house.gov -> https://goldman.house.gov/about\n",
      "Processed: https://malliotakis.house.gov -> https://malliotakis.house.gov/about/meetnicole\n",
      "Processed: https://nadler.house.gov -> https://bsky.app/profile/nadler.house.gov\n",
      "Processed: https://espaillat.house.gov -> https://espaillat.house.gov/about\n",
      "Processed: https://ocasio-cortez.house.gov/ -> https://ocasio-cortez.house.gov/about\n",
      "Processed: https://ritchietorres.house.gov -> Not Found\n",
      "Processed: https://latimer.house.gov/ -> https://latimer.house.gov/about\n",
      "Processed: https://lawler.house.gov -> https://lawler.house.gov/about/\n",
      "Processed: https://patryan.house.gov/ -> https://patryan.house.gov/about\n",
      "Processed: https://riley.house.gov/ -> https://riley.house.gov/about\n",
      "Processed: https://tonko.house.gov/ -> https://bsky.app/profile/reppaultonko.bsky.social\n",
      "Processed: https://stefanik.house.gov/ -> https://stefanik.house.gov/about\n",
      "Processed: https://mannion.house.gov/ -> https://mannion.house.gov/about\n",
      "Processed: https://langworthy.house.gov -> https://langworthy.house.gov/about\n",
      "Processed: https://tenney.house.gov/ -> https://tenney.house.gov/about\n",
      "Processed: https://morelle.house.gov -> https://morelle.house.gov/contact/executiveorders\n",
      "Processed: https://kennedy.house.gov -> https://bsky.app/profile/reptimkennedy.bsky.social\n",
      "Processed: https://dondavis.house.gov -> https://dondavis.house.gov/about\n",
      "Processed: https://ross.house.gov -> https://ross.house.gov/about\n",
      "Processed: https://murphy.house.gov -> https://murphy.house.gov/about/about-greg\n",
      "Processed: https://foushee.house.gov -> https://foushee.house.gov/about\n",
      "Processed: https://foxx.house.gov/ -> https://foxx.house.gov/biography/\n",
      "Processed: https://mcdowell.house.gov/ -> https://mcdowell.house.gov/about\n",
      "Processed: https://rouzer.house.gov/ -> https://rouzer.house.gov/about/biography.htm\n",
      "Processed: https://markharris.house.gov/ -> https://markharris.house.gov/about\n",
      "Processed: https://hudson.house.gov -> https://hudson.house.gov/about\n",
      "Processed: https://harrigan.house.gov/ -> https://harrigan.house.gov/about\n",
      "Processed: https://edwards.house.gov -> https://edwards.house.gov/about\n",
      "Processed: https://adams.house.gov -> https://adams.house.gov/about\n",
      "Processed: https://knott.house.gov/ -> https://knott.house.gov/about\n",
      "Processed: https://timmoore.house.gov/ -> https://timmoore.house.gov/about\n",
      "Processed: https://fedorchak.house.gov/ -> https://fedorchak.house.gov/about\n",
      "Processed: https://king-hinds.house.gov/ -> https://king-hinds.house.gov/about\n",
      "Processed: https://landsman.house.gov -> Not Found\n",
      "Processed: https://taylor.house.gov/ -> https://taylor.house.gov/about\n",
      "Processed: https://beatty.house.gov -> https://beatty.house.gov/full-biography\n",
      "Processed: https://jordan.house.gov/ -> https://jordan.house.gov/about\n",
      "Processed: https://latta.house.gov/ -> https://latta.house.gov/biography\n",
      "Processed: https://rulli.house.gov -> https://rulli.house.gov/about\n",
      "Processed: https://maxmiller.house.gov -> Not Found\n",
      "Processed: https://davidson.house.gov/ -> https://davidson.house.gov/about\n",
      "Processed: https://kaptur.house.gov/ -> https://kaptur.house.gov/about\n",
      "Processed: https://turner.house.gov/ -> https://turner.house.gov/about-mike\n",
      "Processed: https://shontelbrown.house.gov -> https://shontelbrown.house.gov/about\n",
      "Processed: https://balderson.house.gov -> https://balderson.house.gov/about\n",
      "Processed: https://sykes.house.gov -> https://sykes.house.gov/about/bio\n",
      "Processed: https://joyce.house.gov -> Not Found\n",
      "Processed: https://carey.house.gov -> https://carey.house.gov/about/\n",
      "Processed: https://hern.house.gov -> https://hern.house.gov/about/\n",
      "Processed: https://brecheen.house.gov -> https://brecheen.house.gov/about/\n",
      "Processed: https://lucas.house.gov -> Not Found\n",
      "Processed: https://cole.house.gov -> https://cole.house.gov/about\n",
      "Processed: https://bice.house.gov -> https://bice.house.gov/about\n",
      "Processed: https://bonamici.house.gov -> https://bonamici.house.gov/biography\n",
      "Processed: https://bentz.house.gov -> https://bentz.house.gov/about\n",
      "Processed: https://dexter.house.gov/ -> https://dexter.house.gov/about\n",
      "Processed: https://hoyle.house.gov -> https://hoyle.house.gov/about\n",
      "Processed: https://bynum.house.gov/ -> https://bynum.house.gov/about\n",
      "Processed: https://salinas.house.gov -> https://salinas.house.gov/about\n",
      "Processed: https://fitzpatrick.house.gov/ -> https://fitzpatrick.house.gov/about\n",
      "Processed: https://boyle.house.gov/ -> https://boyle.house.gov/about\n",
      "Processed: https://evans.house.gov/ -> https://evans.house.gov/about\n",
      "Processed: https://dean.house.gov -> https://dean.house.gov/about-madeleine\n",
      "Processed: https://scanlon.house.gov/ -> https://scanlon.house.gov/about\n",
      "Processed: https://houlahan.house.gov/ -> https://bsky.app/profile/houlahan.house.gov\n",
      "Processed: https://mackenzie.house.gov/ -> https://mackenzie.house.gov/about\n",
      "Processed: https://bresnahan.house.gov/ -> https://bresnahan.house.gov/about\n",
      "Processed: https://meuser.house.gov -> https://meuser.house.gov/about\n",
      "Processed: https://perry.house.gov/ -> https://perry.house.gov/about/\n",
      "Processed: https://smucker.house.gov/ -> https://smucker.house.gov/about\n",
      "Processed: https://summerlee.house.gov -> Not Found\n",
      "Processed: https://johnjoyce.house.gov/ -> https://johnjoyce.house.gov/about\n",
      "Processed: https://reschenthaler.house.gov/ -> https://reschenthaler.house.gov/about\n",
      "Processed: https://thompson.house.gov -> https://thompson.house.gov/about-gt\n",
      "Processed: https://kelly.house.gov -> https://kelly.house.gov/full-biography\n",
      "Processed: https://deluzio.house.gov -> https://deluzio.house.gov/about\n",
      "Processed: https://hernandez.house.gov/ -> https://hernandez.house.gov/about-pablo-jose\n",
      "Processed: https://amo.house.gov -> https://amo.house.gov/about\n",
      "Processed: https://magaziner.house.gov -> https://magaziner.house.gov/about\n",
      "Processed: https://mace.house.gov -> https://mace.house.gov/about\n",
      "Processed: https://joewilson.house.gov/ -> https://joewilson.house.gov/about/biography\n",
      "Processed: https://sheribiggs.house.gov/ -> https://sheribiggs.house.gov/about\n",
      "Processed: https://timmons.house.gov/ -> https://timmons.house.gov/about/\n",
      "Processed: https://norman.house.gov -> https://norman.house.gov/meet-ralph\n",
      "Processed: https://clyburn.house.gov/ -> https://clyburn.house.gov/biography/\n",
      "Processed: https://fry.house.gov -> https://fry.house.gov/about/\n",
      "Processed: https://dustyjohnson.house.gov/ -> https://dustyjohnson.house.gov/about\n",
      "Processed: https://harshbarger.house.gov -> https://harshbarger.house.gov/about\n",
      "Processed: https://burchett.house.gov -> https://burchett.house.gov/about\n",
      "Processed: https://fleischmann.house.gov/ -> https://fleischmann.house.gov/about-chuck\n",
      "Processed: https://desjarlais.house.gov/ -> https://desjarlais.house.gov/about-scott\n",
      "Processed: https://ogles.house.gov -> https://ogles.house.gov/about\n",
      "Processed: https://johnrose.house.gov/ -> https://johnrose.house.gov/about\n",
      "Processed: https://markgreen.house.gov/ -> https://markgreen.house.gov/about\n",
      "Processed: https://kustoff.house.gov -> https://kustoff.house.gov/about\n",
      "Processed: https://cohen.house.gov/ -> https://cohen.house.gov/about\n",
      "Processed: https://moran.house.gov -> https://moran.house.gov/about/\n",
      "Processed: https://crenshaw.house.gov/ -> https://crenshaw.house.gov/about\n",
      "Processed: https://keithself.house.gov -> https://keithself.house.gov/about\n",
      "Processed: https://fallon.house.gov -> https://fallon.house.gov/about/\n",
      "Processed: https://gooden.house.gov -> https://gooden.house.gov/about/3dda1b23-6d06-48e0-a591-0a5f6689cb15\n",
      "Processed: https://ellzey.house.gov/ -> https://ellzey.house.gov/about\n",
      "Processed: https://fletcher.house.gov -> https://bsky.app/profile/repfletcher.bsky.social\n",
      "Processed: https://luttrell.house.gov -> https://luttrell.house.gov/about\n",
      "Processed: https://algreen.house.gov -> https://algreen.house.gov/about\n",
      "Processed: https://mccaul.house.gov -> https://mccaul.house.gov/about\n",
      "Processed: https://pfluger.house.gov -> https://pfluger.house.gov/about/about-august.htm\n",
      "Processed: https://craiggoldman.house.gov/ -> https://craiggoldman.house.gov/about\n",
      "Processed: https://jackson.house.gov -> https://jackson.house.gov/about/\n",
      "Processed: https://weber.house.gov -> https://weber.house.gov/biography/about-randy.htm\n",
      "Processed: https://delacruz.house.gov -> https://delacruz.house.gov/about/\n",
      "Processed: https://escobar.house.gov -> https://escobar.house.gov/about\n",
      "Processed: https://sessions.house.gov -> https://sessions.house.gov/about\n",
      "Processed: https://clerk.house.gov/members/TX18/vacancy -> https://clerk.house.gov/members\n",
      "Processed: https://arrington.house.gov -> https://arrington.house.gov/about-jodey\n",
      "Processed: https://castro.house.gov -> https://castro.house.gov/about\n",
      "Processed: https://roy.house.gov -> https://roy.house.gov/about\n",
      "Processed: https://nehls.house.gov -> https://nehls.house.gov/about\n",
      "Processed: https://gonzales.house.gov -> https://gonzales.house.gov/about\n",
      "Processed: https://vanduyne.house.gov -> https://vanduyne.house.gov/about\n",
      "Processed: https://williams.house.gov -> https://williams.house.gov/about\n",
      "Processed: https://gill.house.gov/ -> https://gill.house.gov/about\n",
      "Processed: https://cloud.house.gov -> Not Found\n",
      "Processed: https://cuellar.house.gov/ -> https://cuellar.house.gov/biography/\n",
      "Processed: https://sylviagarcia.house.gov/ -> https://sylviagarcia.house.gov/about\n",
      "Processed: https://crockett.house.gov -> https://crockett.house.gov/about\n",
      "Processed: https://carter.house.gov/ -> https://carter.house.gov/help-with-a-federal-agency/hamiltonmoh.htm\n",
      "Processed: https://juliejohnson.house.gov/ -> https://juliejohnson.house.gov/about\n",
      "Processed: https://veasey.house.gov -> https://veasey.house.gov/about/full-biography\n",
      "Processed: https://gonzalez.house.gov -> https://gonzalez.house.gov/about\n",
      "Processed: https://casar.house.gov -> https://casar.house.gov/about\n",
      "Processed: https://babin.house.gov -> https://babin.house.gov/biography/\n",
      "Processed: https://doggett.house.gov -> https://doggett.house.gov/about\n",
      "Processed: https://hunt.house.gov -> https://hunt.house.gov/about\n",
      "Processed: https://blakemoore.house.gov -> https://blakemoore.house.gov/about\n",
      "Processed: https://maloy.house.gov -> https://www.facebook.com/profile.php?id=61555755717517\n",
      "Processed: https://mikekennedy.house.gov/ -> https://mikekennedy.house.gov/about\n",
      "Processed: https://owens.house.gov -> Not Found\n",
      "Processed: https://balint.house.gov -> https://bsky.app/profile/balint.house.gov\n",
      "Processed: https://wittman.house.gov/ -> https://wittman.house.gov/about\n",
      "Processed: https://kiggans.house.gov -> Not Found\n",
      "Processed: https://bobbyscott.house.gov -> https://bobbyscott.house.gov/about/biography\n",
      "Processed: https://mcclellan.house.gov/ -> https://mcclellan.house.gov/about\n",
      "Processed: https://mcguire.house.gov/ -> https://mcguire.house.gov/about\n",
      "Processed: https://cline.house.gov -> https://cline.house.gov/about\n",
      "Processed: https://vindman.house.gov/ -> https://vindman.house.gov/about\n",
      "Processed: https://beyer.house.gov -> https://bsky.app/profile/beyer.house.gov\n",
      "Processed: https://morgangriffith.house.gov/ -> https://morgangriffith.house.gov/contact/meetingrequest.htm\n",
      "Processed: https://subramanyam.house.gov/ -> https://subramanyam.house.gov/about\n",
      "Processed: https://connolly.house.gov/ -> https://connolly.house.gov/biography\n",
      "Processed: https://plaskett.house.gov/ -> https://plaskett.house.gov/biography\n",
      "Processed: https://delbene.house.gov -> https://bsky.app/profile/repdelbene.bsky.social\n",
      "Processed: https://larsen.house.gov -> https://larsen.house.gov/about/\n",
      "Processed: https://gluesenkampperez.house.gov -> Not Found\n",
      "Processed: https://newhouse.house.gov -> https://newhouse.house.gov/about/full-biography\n",
      "Processed: https://baumgartner.house.gov/ -> https://baumgartner.house.gov/about\n",
      "Processed: https://randall.house.gov/ -> https://randall.house.gov/about\n",
      "Processed: https://jayapal.house.gov -> https://jayapal.house.gov/about-me/\n",
      "Processed: https://schrier.house.gov -> https://schrier.house.gov/about\n",
      "Processed: https://adamsmith.house.gov/ -> https://adamsmith.house.gov/about\n",
      "Processed: https://strickland.house.gov -> https://strickland.house.gov/about/\n",
      "Processed: https://miller.house.gov/ -> https://miller.house.gov/about\n",
      "Processed: https://rileymoore.house.gov/ -> https://rileymoore.house.gov/about\n",
      "Processed: https://steil.house.gov -> https://steil.house.gov/about-bryan\n",
      "Processed: https://pocan.house.gov -> https://pocan.house.gov/about\n",
      "Processed: https://vanorden.house.gov -> https://vanorden.house.gov/about\n",
      "Processed: https://gwenmoore.house.gov -> https://gwenmoore.house.gov/biography\n",
      "Processed: https://fitzgerald.house.gov -> https://fitzgerald.house.gov/about\n",
      "Processed: https://grothman.house.gov -> https://grothman.house.gov/biography\n",
      "Processed: https://tiffany.house.gov/ -> https://tiffany.house.gov/about\n",
      "Processed: https://wied.house.gov/ -> https://wied.house.gov/about\n",
      "Processed: https://hageman.house.gov -> https://hageman.house.gov/about\n"
     ]
    }
   ],
   "source": [
    "# Keywords to search for in links\n",
    "keywords = [\"about\", \"biography\", \"bio\", \"full biography\", \"background\", \"profile\", \"who-is\", \"meet-\"]\n",
    "\n",
    "# Function to find about-related URLs from a given URL\n",
    "def find_about_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        filtered_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href').lower()\n",
    "            text = link.get_text().lower()\n",
    "\n",
    "            if any(keyword in href or keyword in text for keyword in keywords):\n",
    "                # Convert relative URLs to absolute URLs\n",
    "                full_url = urljoin(url, href)\n",
    "                filtered_links.append(full_url)\n",
    "\n",
    "        # Return the first valid link found, or None if not found\n",
    "        return filtered_links[0] if filtered_links else None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read URLs from input CSV file\n",
    "input_file = '1homepage.csv'\n",
    "output_file = '2aboutpage.csv'\n",
    "\n",
    "with open(input_file, 'r', encoding='UTF-16') as infile, open(output_file, 'w', newline='', encoding='UTF-16') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    if 'Homepage' not in reader.fieldnames:\n",
    "        raise ValueError(\"Input CSV file must have a column named 'Homepage'\")\n",
    "    \n",
    "    fieldnames = reader.fieldnames + ['candidates official page URL']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        url = row['Homepage'].strip()\n",
    "        \n",
    "        if not url.startswith(\"http\"):\n",
    "            url = \"http://\" + url  # Ensure URL is properly formatted\n",
    "\n",
    "        about_url = find_about_page(url)\n",
    "        row['about-page'] = about_url if about_url else \"Not Found\"\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        print(f\"Processed: {url} -> {about_url if about_url else 'Not Found'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain wikipedia link via Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/441: Moore, Barry Alabama\n",
      "Processing 2/441: Figures, Shomari Alabama\n",
      "Processing 3/441: Rogers, Mike Alabama\n",
      "Processing 4/441: Aderholt, Robert Alabama\n",
      "Processing 5/441: Strong, Dale Alabama\n",
      "Processing 6/441: Palmer, Gary Alabama\n",
      "Processing 7/441: Sewell, Terri Alabama\n",
      "Processing 8/441: Begich, Nicholas Alaska\n",
      "Processing 9/441: Radewagen, Aumua Amata American Samoa\n",
      "Processing 10/441: Schweikert, David Arizona\n",
      "Processing 11/441: Crane, Elijah Arizona\n",
      "Processing 12/441: Ansari, Yassamin Arizona\n",
      "Processing 13/441: Stanton, Greg Arizona\n",
      "Processing 14/441: Biggs, Andy Arizona\n",
      "Processing 15/441: Ciscomani, Juan Arizona\n",
      "Processing 16/441: Grijalva, Ral  Arizona\n",
      "Processing 17/441: Hamadeh, Abraham Arizona\n",
      "Processing 18/441: Gosar, Paul Arizona\n",
      "Processing 19/441: Crawford, Eric Arkansas\n",
      "Processing 20/441: Hill, J. Arkansas\n",
      "Processing 21/441: Womack, Steve Arkansas\n",
      "Processing 22/441: Westerman, Bruce Arkansas\n",
      "Processing 23/441: LaMalfa, Doug California\n",
      "Processing 24/441: Huffman, Jared California\n",
      "Processing 25/441: Kiley, Kevin California\n",
      "Processing 26/441: Thompson, Mike California\n",
      "Processing 27/441: McClintock, Tom California\n",
      "Processing 28/441: Bera, Ami California\n",
      "Processing 29/441: Matsui, Doris California\n",
      "Processing 30/441: Garamendi, John California\n",
      "Processing 31/441: Harder, Josh California\n",
      "Processing 32/441: DeSaulnier, Mark California\n",
      "Processing 33/441: Pelosi, Nancy California\n",
      "Processing 34/441: Simon, Lateefah California\n",
      "Processing 35/441: Gray, Adam California\n",
      "Processing 36/441: Swalwell, Eric California\n",
      "Processing 37/441: Mullin, Kevin California\n",
      "Processing 38/441: Liccardo, Sam California\n",
      "Processing 39/441: Khanna, Ro California\n",
      "Processing 40/441: Lofgren, Zoe California\n",
      "Processing 41/441: Panetta, Jimmy California\n",
      "Processing 42/441: Fong, Vince California\n",
      "Processing 43/441: Costa, Jim California\n",
      "Processing 44/441: Valadao, David California\n",
      "Processing 45/441: Obernolte, Jay California\n",
      "Processing 46/441: Carbajal, Salud California\n",
      "Processing 47/441: Ruiz, Raul California\n",
      "Processing 48/441: Brownley, Julia California\n",
      "Processing 49/441: Whitesides, George California\n",
      "Processing 50/441: Chu, Judy California\n",
      "Processing 51/441: Rivas, Luz California\n",
      "Processing 52/441: Friedman, Laura California\n",
      "Processing 53/441: Cisneros, Gilbert California\n",
      "Processing 54/441: Sherman, Brad California\n",
      "Processing 55/441: Aguilar, Pete California\n",
      "Processing 56/441: Gomez, Jimmy California\n",
      "Processing 57/441: Torres, Norma California\n",
      "Processing 58/441: Lieu, Ted California\n",
      "Processing 59/441: Kamlager-Dove, Sydney California\n",
      "Processing 60/441: Sanchez, Linda California\n",
      "Processing 61/441: Takano, Mark California\n",
      "Processing 62/441: Kim, Young California\n",
      "Processing 63/441: Calvert, Ken California\n",
      "Processing 64/441: Garcia, Robert California\n",
      "Processing 65/441: Waters, Maxine California\n",
      "Processing 66/441: Barragan, Nanette California\n",
      "Processing 67/441: Tran, Derek California\n",
      "Processing 68/441: Correa, J. California\n",
      "Processing 69/441: Min, Dave California\n",
      "Processing 70/441: Issa, Darrell California\n",
      "Processing 71/441: Levin, Mike California\n",
      "Processing 72/441: Peters, Scott California\n",
      "Processing 73/441: Jacobs, Sara California\n",
      "Processing 74/441: Cargas, Juan California\n",
      "Processing 75/441:  Diana, DeGette Colorado\n",
      "Processing 76/441:  Joe, Neguse Colorado\n",
      "Processing 77/441:  Jeff, Hurd Colorado\n",
      "Processing 78/441:  Lauren, Boebert Colorado\n",
      "Processing 79/441:  Jeff, Crank Colorado\n",
      "Processing 80/441:  Jason, Crow Colorado\n",
      "Processing 81/441:  Brittany, Pettersen Colorado\n",
      "Processing 82/441:  Gabe, Evans Colorado\n",
      "Processing 83/441:  John, Larson Connecticut\n",
      "Processing 84/441:  Joe, Courtney Connecticut\n",
      "Processing 85/441:  Rosa, DeLauro Connecticut\n",
      "Processing 86/441:  James, Himes Connecticut\n",
      "Processing 87/441:  Jahana, Hayes Connecticut\n",
      "Processing 88/441:  Sarah, McBride Delaware\n",
      "Processing 89/441:  Eleanor, Norton District of Columbia\n",
      "Processing 90/441:  Matt, Gaetz Florida\n",
      "Processing 91/441:  Neal, Dunn Florida\n",
      "Processing 92/441:  Kat, Cammack Florida\n",
      "Processing 93/441:  Aaron, Bean Florida\n",
      "Processing 94/441:  John, Rutherford Florida\n",
      "Processing 95/441:  Michael, Waltz Florida\n",
      "Processing 96/441:  Cory, Mills Florida\n",
      "Processing 97/441:  Mike, Haridopolos Florida\n",
      "Processing 98/441:  Darren, Soto Florida\n",
      "Processing 99/441:  Maxwell, Frost Florida\n",
      "Processing 100/441:  Daniel, Webster Florida\n",
      "Processing 101/441:  Gus, Bilirakis Florida\n",
      "Processing 102/441:  Anna Paulina, Luna Florida\n",
      "Processing 103/441:  Kathy, Castor Florida\n",
      "Processing 104/441:  Laurel, Lee Florida\n",
      "Processing 105/441:  Vern, Buchanan Florida\n",
      "Processing 106/441:  W., Steube Florida\n",
      "Processing 107/441:  Scott, Franklin Florida\n",
      "Processing 108/441:  Byron, Donalds Florida\n",
      "Processing 109/441:  Sheila, Cherfilus-McCormick Florida\n",
      "Processing 110/441:  Brian, Mast Florida\n",
      "Processing 111/441:  Lois, Frankel Florida\n",
      "Processing 112/441:  Jared, Moskowitz Florida\n",
      "Processing 113/441:  Frederica, Wilson Florida\n",
      "Processing 114/441:  Debbie, Wasserman Schultz Florida\n",
      "Processing 115/441:  Mario, Diaz-Balart Florida\n",
      "Processing 116/441:  Maria, Salazar Florida\n",
      "Processing 117/441:  Carlos, Gimenez Florida\n",
      "Processing 118/441:  Earl, Carter Georgia\n",
      "Processing 119/441:  Sanford, Bishop Georgia\n",
      "Processing 120/441:  Brian, Jack Georgia\n",
      "Processing 121/441:  Henry, Johnson Georgia\n",
      "Processing 122/441:  Nikema, Williams Georgia\n",
      "Processing 123/441:  Lucy, McBath Georgia\n",
      "Processing 124/441:  Richard, McCormick Georgia\n",
      "Processing 125/441:  Austin, Scott Georgia\n",
      "Processing 126/441:  Andrew, Clyde Georgia\n",
      "Processing 127/441:  Mike, Collins Georgia\n",
      "Processing 128/441:  Barry, Loudermilk Georgia\n",
      "Processing 129/441:  Rick, Allen Georgia\n",
      "Processing 130/441:  David, Scott Georgia\n",
      "Processing 131/441:  Marjorie, Greene Georgia\n",
      "Processing 132/441:  James, Moylan Guam\n",
      "Processing 133/441:  Ed, Case Hawaii\n",
      "Processing 134/441:  Jill, Tokuda Hawaii\n",
      "Processing 135/441:  Russ, Fulcher Idaho\n",
      "Processing 136/441:  Michael, Simpson Idaho\n",
      "Processing 137/441:  Jonathan, Jackson Illinois\n",
      "Processing 138/441:  Robin, Kelly Illinois\n",
      "Processing 139/441:  Delia, Ramirez Illinois\n",
      "Processing 140/441:  Jesus, Garcia Illinois\n",
      "Processing 141/441:  Mike, Quigley Illinois\n",
      "Processing 142/441:  Sean, Casten Illinois\n",
      "Processing 143/441:  Danny, Davis Illinois\n",
      "Processing 144/441:  Raja, Krishnamoorthi Illinois\n",
      "Processing 145/441:  Janice, Schakowsky Illinois\n",
      "Processing 146/441:  Bradley, Schneider Illinois\n",
      "Processing 147/441:  Bill, Foster Illinois\n",
      "Processing 148/441:  Mike, Bost Illinois\n",
      "Processing 149/441:  Nikki, Budzinski Illinois\n",
      "Processing 150/441:  Lauren, Underwood Illinois\n",
      "Processing 151/441:  Mary, Miller Illinois\n",
      "Processing 152/441:  Darin, LaHood Illinois\n",
      "Processing 153/441:  Eric, Sorensen Illinois\n",
      "Processing 154/441:  Frank, Mrvan Indiana\n",
      "Processing 155/441:  Rudy, Yakym Indiana\n",
      "Processing 156/441:  Marlin, Stutzman Indiana\n",
      "Processing 157/441:  James, Baird Indiana\n",
      "Processing 158/441:  Victoria, Spartz Indiana\n",
      "Processing 159/441:  Jefferson, Shreve Indiana\n",
      "Processing 160/441:  Andre, Carson Indiana\n",
      "Processing 161/441:  Mark, Messmer Indiana\n",
      "Processing 162/441:  Erin, Houchin Indiana\n",
      "Processing 163/441:  Mariannette, Miller-Meeks Iowa\n",
      "Processing 164/441:  Ashley, Hinson Iowa\n",
      "Processing 165/441:  Zachary, Nunn Iowa\n",
      "Processing 166/441:  Randy, Feenstra Iowa\n",
      "Processing 167/441:  Tracey, Mann Kansas\n",
      "Processing 168/441:  Derek, Schmidt Kansas\n",
      "Processing 169/441:  Sharice, Davids Kansas\n",
      "Processing 170/441:  Ron, Estes Kansas\n",
      "Processing 171/441:  James, Comer Kentucky\n",
      "Processing 172/441:  Brett, Guthrie Kentucky\n",
      "Processing 173/441:  Morgan, McGarvey Kentucky\n",
      "Processing 174/441:  Thomas, Massie Kentucky\n",
      "Processing 175/441:  Harold, Rogers Kentucky\n",
      "Processing 176/441:  Andy, Barr Kentucky\n",
      "Processing 177/441:  Steve, Scalise Louisiana\n",
      "Processing 178/441:  Troy, Carter Louisiana\n",
      "Processing 179/441:  Clay, Higgins Louisiana\n",
      "Processing 180/441:  Mike, Johnson Louisiana\n",
      "Processing 181/441:  Julia, Letlow Louisiana\n",
      "Processing 182/441:  Cleo, Fields Louisiana\n",
      "Processing 183/441:  Chellie, Pingree Maine\n",
      "Processing 184/441:  Jared, Golden Maine\n",
      "Processing 185/441:  Andy, Harris Maryland\n",
      "Processing 186/441:  Johnny, Olszewski Maryland\n",
      "Processing 187/441:  Sarah, Elfreth Maryland\n",
      "Processing 188/441:  Glenn, Ivey Maryland\n",
      "Processing 189/441:  Steny, Hoyer Maryland\n",
      "Processing 190/441:  April, McClain Delaney Maryland\n",
      "Processing 191/441:  Kweisi, Mfume Maryland\n",
      "Processing 192/441:  Jamie, Raskin Maryland\n",
      "Processing 193/441:  Richard, Neal Massachusetts\n",
      "Processing 194/441:  James, McGovern Massachusetts\n",
      "Processing 195/441:  Lori, Trahan Massachusetts\n",
      "Processing 196/441:  Jake, Auchincloss Massachusetts\n",
      "Processing 197/441:  Katherine, Clark Massachusetts\n",
      "Processing 198/441:  Seth, Moulton Massachusetts\n",
      "Processing 199/441:  Ayanna, Pressley Massachusetts\n",
      "Processing 200/441:  Stephen, Lynch Massachusetts\n",
      "Processing 201/441:  William, Keating Massachusetts\n",
      "Processing 202/441:  Jack, Bergman Michigan\n",
      "Processing 203/441:  John, Moolenaar Michigan\n",
      "Processing 204/441:  Hillary, Scholten Michigan\n",
      "Processing 205/441:  Bill, Huizenga Michigan\n",
      "Processing 206/441:  Tim, Walberg Michigan\n",
      "Processing 207/441:  Debbie, Dingell Michigan\n",
      "Processing 208/441:  Tom, Barrett Michigan\n",
      "Processing 209/441:  Kristen, McDonald Rivet Michigan\n",
      "Processing 210/441:  Lisa, McClain Michigan\n",
      "Processing 211/441:  John, James Michigan\n",
      "Processing 212/441:  Haley, Stevens Michigan\n",
      "Processing 213/441:  Rashida, Tlaib Michigan\n",
      "Processing 214/441:  Shri, Thanedar Michigan\n",
      "Processing 215/441:  Brad, Finstad Minnesota\n",
      "Processing 216/441:  Angie, Craig Minnesota\n",
      "Processing 217/441:  Kelly, Morrison Minnesota\n",
      "Processing 218/441:  Betty, McCollum Minnesota\n",
      "Processing 219/441:  Ilhan, Omar Minnesota\n",
      "Processing 220/441:  Tom, Emmer Minnesota\n",
      "Processing 221/441:  Michelle, Fischbach Minnesota\n",
      "Processing 222/441:  Pete, Stauber Minnesota\n",
      "Processing 223/441:  Trent, Kelly Mississippi\n",
      "Processing 224/441:  Bennie, Thompson Mississippi\n",
      "Processing 225/441:  Michael, Guest Mississippi\n",
      "Processing 226/441:  Mike, Ezell Mississippi\n",
      "Processing 227/441:  Wesley, Bell Missouri\n",
      "Processing 228/441:  Ann, Wagner Missouri\n",
      "Processing 229/441:  Robert, Onder Missouri\n",
      "Processing 230/441:  Mark, Alford Missouri\n",
      "Processing 231/441:  Emanuel, Cleaver Missouri\n",
      "Processing 232/441:  Sam, Graves Missouri\n",
      "Processing 233/441:  Eric, Burlison Missouri\n",
      "Processing 234/441:  Jason, Smith Missouri\n",
      "Processing 235/441:  Ryan, Zinke Montana\n",
      "Processing 236/441:  Troy, Downing Montana\n",
      "Processing 237/441:  Mike, Flood Nebraska\n",
      "Processing 238/441:  Don, Bacon Nebraska\n",
      "Processing 239/441:  Adrian, Smith Nebraska\n",
      "Processing 240/441:  Dina, Titus Nevada\n",
      "Processing 241/441:  Mark, Amodei Nevada\n",
      "Processing 242/441:  Susie, Lee Nevada\n",
      "Processing 243/441:  Steven, Horsford Nevada\n",
      "Processing 244/441:  Chris, Pappas New Hampshire\n",
      "Processing 245/441:  Maggie, Goodlander New Hampshire\n",
      "Processing 246/441:  Donald, Norcross New Jersey\n",
      "Processing 247/441:  Jefferson, Van Drew New Jersey\n",
      "Processing 248/441:  Herbert, Conaway New Jersey\n",
      "Processing 249/441:  Christopher, Smith New Jersey\n",
      "Processing 250/441:  Josh, Gottheimer New Jersey\n",
      "Processing 251/441:  Frank, Pallone New Jersey\n",
      "Processing 252/441:  Thomas, Kean New Jersey\n",
      "Processing 253/441:  Robert, Menendez New Jersey\n",
      "Processing 254/441:  Nellie, Pou New Jersey\n",
      "Processing 255/441:  LaMonica, McIver New Jersey\n",
      "Processing 256/441:  Mikie, Sherrill New Jersey\n",
      "Processing 257/441:  Bonnie, Watson Coleman New Jersey\n",
      "Processing 258/441:  Melanie, Stansbury New Mexico\n",
      "Processing 259/441:  Gabe, Vasquez New Mexico\n",
      "Processing 260/441:  Teresa, Leger Fernandez New Mexico\n",
      "Processing 261/441:  Nick, LaLota New York\n",
      "Processing 262/441:  Andrew, Garbarino New York\n",
      "Processing 263/441:  Thomas R., Suozzi New York\n",
      "Processing 264/441:  Laura, Gillen New York\n",
      "Processing 265/441:  Gregory, Meeks New York\n",
      "Processing 266/441:  Grace, Meng New York\n",
      "Processing 267/441:  Nydia, Velazquez New York\n",
      "Processing 268/441:  Hakeem, Jeffries New York\n",
      "Processing 269/441:  Yvette, Clarke New York\n",
      "Processing 270/441:  Daniel, Goldman New York\n",
      "Processing 271/441:  Nicole, Malliotakis New York\n",
      "Processing 272/441:  Jerrold, Nadler New York\n",
      "Processing 273/441:  Adriano, Espaillat New York\n",
      "Processing 274/441:  Alexandria, Ocasio-Cortez New York\n",
      "Processing 275/441:  Ritchie, Torres New York\n",
      "Processing 276/441:  George, Latimer New York\n",
      "Processing 277/441:  Michael, Lawler New York\n",
      "Processing 278/441:  Patrick, Ryan New York\n",
      "Processing 279/441:  Josh, Riley New York\n",
      "Processing 280/441:  Paul, Tonko New York\n",
      "Processing 281/441:  Elise, Stefanik New York\n",
      "Processing 282/441:  John, Mannion New York\n",
      "Processing 283/441:  Nicholas, Langworthy New York\n",
      "Processing 284/441:  Claudia, Tenney New York\n",
      "Processing 285/441:  Joseph, Morelle New York\n",
      "Processing 286/441:  Timothy, Kennedy New York\n",
      "Processing 287/441:  Donald, Davis North Carolina\n",
      "Processing 288/441:  Deborah, Ross North Carolina\n",
      "Processing 289/441:  Gregory, Murphy North Carolina\n",
      "Processing 290/441:  Valerie, Foushee North Carolina\n",
      "Processing 291/441:  Virginia, Foxx North Carolina\n",
      "Processing 292/441:  Addison, McDowell North Carolina\n",
      "Processing 293/441:  David, Rouzer North Carolina\n",
      "Processing 294/441:  Mark, Harris North Carolina\n",
      "Processing 295/441:  Richard, Hudson North Carolina\n",
      "Processing 296/441:  Pat, Harrigan North Carolina\n",
      "Processing 297/441:  Chuck, Edwards North Carolina\n",
      "Processing 298/441:  Alma, Adams North Carolina\n",
      "Processing 299/441:  Brad, Knott North Carolina\n",
      "Processing 300/441:  Tim, Moore North Carolina\n",
      "Processing 301/441:  Julie, Fedorchak North Dakota\n",
      "Processing 302/441:  Kimberlyn, King-Hinds Northern Mariana Islands\n",
      "Processing 303/441:  Greg, Landsman Ohio\n",
      "Processing 304/441:  David, Taylor Ohio\n",
      "Processing 305/441:  Joyce, Beatty Ohio\n",
      "Processing 306/441:  Jim, Jordan Ohio\n",
      "Processing 307/441:  Robert, Latta Ohio\n",
      "Processing 308/441:  Michael A., Rulli Ohio\n",
      "Processing 309/441:  Max, Miller Ohio\n",
      "Processing 310/441:  Warren, Davidson Ohio\n",
      "Processing 311/441:  Marcy, Kaptur Ohio\n",
      "Processing 312/441:  Michael, Turner Ohio\n",
      "Processing 313/441:  Shontel, Brown Ohio\n",
      "Processing 314/441:  Troy, Balderson Ohio\n",
      "Processing 315/441:  Emilia, Sykes Ohio\n",
      "Processing 316/441:  David, Joyce Ohio\n",
      "Processing 317/441:  Mike, Carey Ohio\n",
      "Processing 318/441:  Kevin, Hern Oklahoma\n",
      "Processing 319/441:  Josh, Brecheen Oklahoma\n",
      "Processing 320/441:  Frank, Lucas Oklahoma\n",
      "Processing 321/441:  Tom, Cole Oklahoma\n",
      "Processing 322/441:  Stephanie, Bice Oklahoma\n",
      "Processing 323/441:  Suzanne, Bonamici Oregon\n",
      "Processing 324/441:  Cliff, Bentz Oregon\n",
      "Processing 325/441:  Maxine, Dexter Oregon\n",
      "Processing 326/441:  Val, Hoyle Oregon\n",
      "Processing 327/441:  Janelle, Bynum Oregon\n",
      "Processing 328/441:  Andrea, Salinas Oregon\n",
      "Processing 329/441:  Brian, Fitzpatrick Pennsylvania\n",
      "Processing 330/441:  Brendan, Boyle Pennsylvania\n",
      "Processing 331/441:  Dwight, Evans Pennsylvania\n",
      "Processing 332/441:  Madeleine, Dean Pennsylvania\n",
      "Processing 333/441:  Mary Gay, Scanlon Pennsylvania\n",
      "Processing 334/441:  Chrissy, Houlahan Pennsylvania\n",
      "Processing 335/441:  Ryan, Mackenzie Pennsylvania\n",
      "Processing 336/441:  Robert, Bresnahan Pennsylvania\n",
      "Processing 337/441:  Daniel, Meuser Pennsylvania\n",
      "Processing 338/441:  Scott, Perry Pennsylvania\n",
      "Processing 339/441:  Lloyd, Smucker Pennsylvania\n",
      "Processing 340/441:  Summer, Lee Pennsylvania\n",
      "Processing 341/441:  John, Joyce Pennsylvania\n",
      "Processing 342/441:  Guy, Reschenthaler Pennsylvania\n",
      "Processing 343/441:  Glenn, Thompson Pennsylvania\n",
      "Processing 344/441:  Mike, Kelly Pennsylvania\n",
      "Processing 345/441:  Christopher, Deluzio Pennsylvania\n",
      "Processing 346/441:  Pablo, Hernandez Puerto Rico\n",
      "Processing 347/441:  Gabe, Amo Rhode Island\n",
      "Processing 348/441:  Seth, Magaziner Rhode Island\n",
      "Processing 349/441:  Nancy, Mace South Carolina\n",
      "Processing 350/441:  Joe, Wilson South Carolina\n",
      "Processing 351/441:  Sheri, Biggs South Carolina\n",
      "Processing 352/441:  William, Timmons South Carolina\n",
      "Processing 353/441:  Ralph, Norman South Carolina\n",
      "Processing 354/441:  James, Clyburn South Carolina\n",
      "Processing 355/441:  Russell, Fry South Carolina\n",
      "Processing 356/441:  Dusty, Johnson South Dakota\n",
      "Processing 357/441:  Diana, Harshbarger Tennessee\n",
      "Processing 358/441:  Tim, Burchett Tennessee\n",
      "Processing 359/441:  Charles, Fleischmann Tennessee\n",
      "Processing 360/441:  Scott, DesJarlais Tennessee\n",
      "Processing 361/441:  Andrew, Ogles Tennessee\n",
      "Processing 362/441:  John, Rose Tennessee\n",
      "Processing 363/441:  Mark, Green Tennessee\n",
      "Processing 364/441:  David, Kustoff Tennessee\n",
      "Processing 365/441:  Steve, Cohen Tennessee\n",
      "Processing 366/441:  Nathaniel, Moran Texas\n",
      "Processing 367/441:  Dan, Crenshaw Texas\n",
      "Processing 368/441:  Keith, Self Texas\n",
      "Processing 369/441:  Pat, Fallon Texas\n",
      "Processing 370/441:  Lance, Gooden Texas\n",
      "Processing 371/441:  Jake, Ellzey Texas\n",
      "Processing 372/441:  Lizzie, Fletcher Texas\n",
      "Processing 373/441:  Morgan, Luttrell Texas\n",
      "Processing 374/441:  Al, Green Texas\n",
      "Processing 375/441:  Michael, McCaul Texas\n",
      "Processing 376/441:  August, Pfluger Texas\n",
      "Processing 377/441:  Craig, Goldman Texas\n",
      "Processing 378/441:  Ronny, Jackson Texas\n",
      "Processing 379/441:  Randy, Weber Texas\n",
      "Processing 380/441:  Monica, De La Cruz Texas\n",
      "Processing 381/441:  Veronica, Escobar Texas\n",
      "Processing 382/441:  Pete, Sessions Texas\n",
      "Processing 383/441:  Sylvester, Turner Texas\n",
      "Processing 384/441:  Jodey, Arrington Texas\n",
      "Processing 385/441:  Joaquin, Castro Texas\n",
      "Processing 386/441:  Chip, Roy Texas\n",
      "Processing 387/441:  Troy, Nehls Texas\n",
      "Processing 388/441:  Tony, Gonzales Texas\n",
      "Processing 389/441:  Beth, Van Duyne Texas\n",
      "Processing 390/441:  Roger, Williams Texas\n",
      "Processing 391/441:  Brandon, Gill Texas\n",
      "Processing 392/441:  Michael, Cloud Texas\n",
      "Processing 393/441:  Henry, Cuellar Texas\n",
      "Processing 394/441:  Sylvia, Garcia Texas\n",
      "Processing 395/441:  Jasmine, Crockett Texas\n",
      "Processing 396/441:  John, Carter Texas\n",
      "Processing 397/441:  Julie, Johnson Texas\n",
      "Processing 398/441:  Marc, Veasey Texas\n",
      "Processing 399/441:  Vicente, Gonzalez Texas\n",
      "Processing 400/441:  Greg, Casar Texas\n",
      "Processing 401/441:  Brian, Babin Texas\n",
      "Processing 402/441:  Lloyd, Doggett Texas\n",
      "Processing 403/441:  Wesley, Hunt Texas\n",
      "Processing 404/441:  Blake, Moore Utah\n",
      "Processing 405/441:  Celeste, Maloy Utah\n",
      "Processing 406/441:  Mike, Kennedy Utah\n",
      "Processing 407/441:  Burgess, Owens Utah\n",
      "Processing 408/441:  Becca, Balint Vermont\n",
      "Processing 409/441:  Robert, Wittman Virginia\n",
      "Processing 410/441:  Jennifer, Kiggans Virginia\n",
      "Processing 411/441:  Robert, Scott Virginia\n",
      "Processing 412/441:  Jennifer, McClellan Virginia\n",
      "Processing 413/441:  John, McGuire Virginia\n",
      "Processing 414/441:  Ben, Cline Virginia\n",
      "Processing 415/441:  Eugene, Vindman Virginia\n",
      "Processing 416/441:  Donald, Beyer Virginia\n",
      "Processing 417/441:  H., Griffith Virginia\n",
      "Processing 418/441:  Suhas, Subramanyam Virginia\n",
      "Processing 419/441:  Gerald, Connolly Virginia\n",
      "Processing 420/441:  Stacey, Plaskett Virgin Islands\n",
      "Processing 421/441:  Suzan, DelBene Washington\n",
      "Processing 422/441:  Rick, Larsen Washington\n",
      "Processing 423/441:  Marie, Perez Washington\n",
      "Processing 424/441:  Dan, Newhouse Washington\n",
      "Processing 425/441:  Michael, Baumgartner Washington\n",
      "Processing 426/441:  Emily, Randall Washington\n",
      "Processing 427/441:  Pramila, Jayapal Washington\n",
      "Processing 428/441:  Kim, Schrier Washington\n",
      "Processing 429/441:  Adam, Smith Washington\n",
      "Processing 430/441:  Marilyn, Strickland Washington\n",
      "Processing 431/441:  Carol, Miller West Virginia\n",
      "Processing 432/441:  Riley, Moore West Virginia\n",
      "Processing 433/441: Steil, Bryan Wisconsin\n",
      "Processing 434/441: Pocan, Mark Wisconsin\n",
      "Processing 435/441: Van Orden, Derrick Wisconsin\n",
      "Processing 436/441: Moore, Gwen Wisconsin\n",
      "Processing 437/441: Fitzgerald, Scott Wisconsin\n",
      "Processing 438/441: Grothman, Glenn Wisconsin\n",
      "Processing 439/441: Tiffany, Thomas Wisconsin\n",
      "Processing 440/441: Wied, Tony Wisconsin\n",
      "Processing 441/441: Hageman, Harriet Wyoming\n",
      "Processing complete. Results saved to aboutpage-n-wiki.csv\n"
     ]
    }
   ],
   "source": [
    "def find_wikipedia_url(candidate_name, canidate_state, about_page_url=None):\n",
    "    \"\"\"\n",
    "    Search Wikipedia for a politician and return the most likely Wikipedia page URL.\n",
    "    If a valid about_page_url is provided and contains a Wikipedia link, it is returned directly.\n",
    "    \"\"\"\n",
    "    if about_page_url and 'wikipedia.org/wiki/' in about_page_url:\n",
    "        return about_page_url\n",
    "    # Prepare the search query\n",
    "    search_query = \"\\\"\" + candidate_name + \"\\\"\" + canidate_state + \" politician\"\n",
    "    encoded_query = urllib.parse.quote(search_query)\n",
    "    \n",
    "    # Try the Google search method first\n",
    "    try:\n",
    "        google_search_url = f\"https://www.google.com/search?q={encoded_query}+site:wikipedia.org\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(google_search_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the first Wikipedia link in Google results\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if 'wikipedia.org/wiki/' in href:\n",
    "                start = href.find('url=') + 4 if 'url=' in href else 0\n",
    "                end = href.find('&', start) if '&' in href[start:] else len(href)\n",
    "                wikipedia_url = href[start:end]\n",
    "                \n",
    "                if 'wikipedia.org/wiki/' in wikipedia_url:\n",
    "                    return wikipedia_url.split('&')[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # If Google method fails, try Wikipedia's API directly\n",
    "    try:\n",
    "        api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'search',\n",
    "            'srsearch': f\"{candidate_name} {canidate_state} politician senate house\", # <-- change search query here\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(api_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data['query']['search']:\n",
    "            page_title = data['query']['search'][0]['title']\n",
    "            wikipedia_url = f\"https://en.wikipedia.org/wiki/{urllib.parse.quote(page_title.replace(' ', '_'))}\"\n",
    "            return wikipedia_url\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "def process_candidates(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process the input CSV file and add Wikipedia URLs.\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Add a new column for Wikipedia URLs\n",
    "    df['wikipedia_url'] = \"\"\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        candidate_name = row[\"Name\"] if \"Name\" in df.columns else \"\"\n",
    "        canidate_state = row[\"State\"] if \"State\" in df.columns else \"\"\n",
    "        about_page = row['about-page'] if 'about-page' in df.columns else \"\"\n",
    "        \n",
    "        print(f\"Processing {index + 1}/{len(df)}: {candidate_name} {canidate_state}\")\n",
    "        \n",
    "        wikipedia_url = find_wikipedia_url(candidate_name, canidate_state, about_page)\n",
    "        df.at[index, 'wikipedia_url'] = wikipedia_url\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Save to new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processing complete. Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"2aboutpage.csv\"\n",
    "    output_filename = \"3aboutpage-n-wiki.csv\"\n",
    "    \n",
    "    process_candidates(input_filename, output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki vcard scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Wikipedia scrape for 100 URLs...\n",
      "Processing URL 1/100: https://en.wikipedia.org/wiki/Todd_Young\n",
      "Processing URL 2/100: https://en.wikipedia.org/wiki/Ron_Wyden\n",
      "Processing URL 3/100: https://en.wikipedia.org/wiki/Roger_Wicker\n",
      "Processing URL 4/100: https://en.wikipedia.org/wiki/Sheldon_Whitehouse\n",
      "Processing URL 5/100: https://en.wikipedia.org/wiki/Peter_Welch\n",
      "Processing URL 6/100: https://en.wikipedia.org/wiki/Elizabeth_Warren\n",
      "Processing URL 7/100: https://en.wikipedia.org/wiki/Raphael_Warnock\n",
      "Processing URL 8/100: https://en.wikipedia.org/wiki/Mark_Warner\n",
      "Processing URL 9/100: https://en.wikipedia.org/wiki/Chris_Van_Hollen\n",
      "Processing URL 10/100: https://en.wikipedia.org/wiki/Tommy_Tuberville\n",
      "Processing URL 11/100: https://en.wikipedia.org/wiki/Thom_Tillis\n",
      "Processing URL 12/100: https://en.wikipedia.org/wiki/John_Thune\n",
      "Processing URL 13/100: https://en.wikipedia.org/wiki/Dan_Sullivan_(U.S._senator)\n",
      "Processing URL 14/100: https://en.wikipedia.org/wiki/Tina_Smith\n",
      "Processing URL 15/100: https://en.wikipedia.org/wiki/Elissa_Slotkin\n",
      "Processing URL 16/100: https://en.wikipedia.org/wiki/Tim_Sheehy\n",
      "Processing URL 17/100: https://en.wikipedia.org/wiki/Jeanne_Shaheen\n",
      "Processing URL 18/100: https://en.wikipedia.org/wiki/Tim_Scott\n",
      "Processing URL 19/100: https://en.wikipedia.org/wiki/Rick_Scott\n",
      "Processing URL 20/100: https://en.wikipedia.org/wiki/Chuck_Schumer\n",
      "Processing URL 21/100: https://en.wikipedia.org/wiki/Eric_Schmitt\n",
      "Processing URL 22/100: https://en.wikipedia.org/wiki/Adam_Schiff\n",
      "Processing URL 23/100: https://en.wikipedia.org/wiki/Brian_Schatz\n",
      "Processing URL 24/100: https://en.wikipedia.org/wiki/Bernie_Sanders\n",
      "Processing URL 25/100: https://en.wikipedia.org/wiki/Mike_Rounds\n",
      "Processing URL 26/100: https://en.wikipedia.org/wiki/Jacky_Rosen\n",
      "Processing URL 27/100: https://en.wikipedia.org/wiki/Jim_Risch\n",
      "Processing URL 28/100: https://en.wikipedia.org/wiki/Pete_Ricketts\n",
      "Processing URL 29/100: https://en.wikipedia.org/wiki/Jack_Reed_(Rhode_Island_politician)\n",
      "Processing URL 30/100: https://en.wikipedia.org/wiki/Gary_Peters\n",
      "Processing URL 31/100: https://en.wikipedia.org/wiki/Rand_Paul\n",
      "Processing URL 32/100: https://en.wikipedia.org/wiki/Alex_Padilla\n",
      "Processing URL 33/100: https://en.wikipedia.org/wiki/Jon_Ossoff\n",
      "Processing URL 34/100: https://en.wikipedia.org/wiki/Patty_Murray\n",
      "Processing URL 35/100: https://en.wikipedia.org/wiki/Chris_Murphy\n",
      "Processing URL 36/100: https://en.wikipedia.org/wiki/Lisa_Murkowski\n",
      "Processing URL 37/100: https://en.wikipedia.org/wiki/Markwayne_Mullin\n",
      "Processing URL 38/100: https://en.wikipedia.org/wiki/Bernie_Moreno\n",
      "Processing URL 39/100: https://en.wikipedia.org/wiki/Jerry_Moran\n",
      "Processing URL 40/100: https://en.wikipedia.org/wiki/Ashley_Moody\n",
      "Processing URL 41/100: https://en.wikipedia.org/wiki/Jeff_Merkley\n",
      "Processing URL 42/100: https://en.wikipedia.org/wiki/David_McCormick\n",
      "Processing URL 43/100: https://en.wikipedia.org/wiki/Mitch_McConnell\n",
      "Processing URL 44/100: https://en.wikipedia.org/wiki/Roger_Marshall\n",
      "Processing URL 45/100: https://en.wikipedia.org/wiki/Ed_Markey\n",
      "Processing URL 46/100: https://en.wikipedia.org/wiki/Cynthia_Lummis\n",
      "Processing URL 47/100: https://en.wikipedia.org/wiki/Ben_Ray_Luj%C3%A1n\n",
      "Processing URL 48/100: https://en.wikipedia.org/wiki/Mike_Lee\n",
      "Processing URL 49/100: https://en.wikipedia.org/wiki/James_Lankford\n",
      "Processing URL 50/100: https://en.wikipedia.org/wiki/Amy_Klobuchar\n",
      "Processing URL 51/100: https://en.wikipedia.org/wiki/Angus_King\n",
      "Processing URL 52/100: https://en.wikipedia.org/wiki/Andy_Kim\n",
      "Processing URL 53/100: https://en.wikipedia.org/wiki/John_Neely_Kennedy\n",
      "Processing URL 54/100: https://en.wikipedia.org/wiki/Mark_Kelly\n",
      "Processing URL 55/100: https://en.wikipedia.org/wiki/Tim_Kaine\n",
      "Processing URL 56/100: https://en.wikipedia.org/wiki/Jim_Justice\n",
      "Processing URL 57/100: https://en.wikipedia.org/wiki/Ron_Johnson\n",
      "Processing URL 58/100: https://en.wikipedia.org/wiki/Cindy_Hyde-Smith\n",
      "Processing URL 59/100: https://en.wikipedia.org/wiki/Jon_Husted\n",
      "Processing URL 60/100: https://en.wikipedia.org/wiki/John_Hoeven\n",
      "Processing URL 61/100: https://en.wikipedia.org/wiki/Mazie_Hirono\n",
      "Processing URL 62/100: https://en.wikipedia.org/wiki/John_Hickenlooper\n",
      "Processing URL 63/100: https://en.wikipedia.org/wiki/Martin_Heinrich\n",
      "Processing URL 64/100: https://en.wikipedia.org/wiki/Josh_Hawley\n",
      "Processing URL 65/100: https://en.wikipedia.org/wiki/Maggie_Hassan\n",
      "Processing URL 66/100: https://en.wikipedia.org/wiki/Bill_Hagerty\n",
      "Processing URL 67/100: https://en.wikipedia.org/wiki/Chuck_Grassley\n",
      "Processing URL 68/100: https://en.wikipedia.org/wiki/Lindsey_Graham\n",
      "Processing URL 69/100: https://en.wikipedia.org/wiki/Kirsten_Gillibrand\n",
      "Processing URL 70/100: https://en.wikipedia.org/wiki/Ruben_Gallego\n",
      "Processing URL 71/100: https://en.wikipedia.org/wiki/Deb_Fischer\n",
      "Processing URL 72/100: https://en.wikipedia.org/wiki/John_Fetterman\n",
      "Processing URL 73/100: https://en.wikipedia.org/wiki/Joni_Ernst\n",
      "Processing URL 74/100: https://en.wikipedia.org/wiki/Dick_Durbin\n",
      "Processing URL 75/100: https://en.wikipedia.org/wiki/Tammy_Duckworth\n",
      "Processing URL 76/100: https://en.wikipedia.org/wiki/Steve_Daines\n",
      "Processing URL 77/100: https://en.wikipedia.org/wiki/John_Curtis\n",
      "Processing URL 78/100: https://en.wikipedia.org/wiki/Ted_Cruz\n",
      "Processing URL 79/100: https://en.wikipedia.org/wiki/Mike_Crapo\n",
      "Processing URL 80/100: https://en.wikipedia.org/wiki/Kevin_Cramer\n",
      "Processing URL 81/100: https://en.wikipedia.org/wiki/Tom_Cotton\n",
      "Processing URL 82/100: https://en.wikipedia.org/wiki/Catherine_Cortez_Masto\n",
      "Processing URL 83/100: https://en.wikipedia.org/wiki/John_Cornyn\n",
      "Processing URL 84/100: https://en.wikipedia.org/wiki/Chris_Coons\n",
      "Processing URL 85/100: https://en.wikipedia.org/wiki/Susan_Collins\n",
      "Processing URL 86/100: https://en.wikipedia.org/wiki/Bill_Cassidy\n",
      "Processing URL 87/100: https://en.wikipedia.org/wiki/Shelley_Moore_Capito\n",
      "Processing URL 88/100: https://en.wikipedia.org/wiki/Maria_Cantwell\n",
      "Processing URL 89/100: https://en.wikipedia.org/wiki/Ted_Budd\n",
      "Processing URL 90/100: https://en.wikipedia.org/wiki/Katie_Britt\n",
      "Processing URL 91/100: https://en.wikipedia.org/wiki/John_Boozman\n",
      "Processing URL 92/100: https://en.wikipedia.org/wiki/Cory_Booker\n",
      "Processing URL 93/100: https://en.wikipedia.org/wiki/Lisa_Blunt_Rochester\n",
      "Processing URL 94/100: https://en.wikipedia.org/wiki/Richard_Blumenthal\n",
      "Processing URL 95/100: https://en.wikipedia.org/wiki/Marsha_Blackburn\n",
      "Processing URL 96/100: https://en.wikipedia.org/wiki/Michael_Bennet\n",
      "Processing URL 97/100: https://en.wikipedia.org/wiki/John_Barrasso\n",
      "Processing URL 98/100: https://en.wikipedia.org/wiki/Jim_Banks\n",
      "Processing URL 99/100: https://en.wikipedia.org/wiki/Tammy_Baldwin\n",
      "Processing URL 100/100: https://en.wikipedia.org/wiki/Angela_Alsobrooks\n",
      "Scraping complete. Adding results to DataFrame...\n",
      "Extraction complete. Results saved to 4aboutpage-n-wiki-vcard.csv\n"
     ]
    }
   ],
   "source": [
    "def get_info_from_wikipedia(url):\n",
    "    \"\"\"\n",
    "    Scrapes a Wikipedia URL's infobox for Education/Alma Mater and\n",
    "    Assumed Office/In Office dates.\n",
    "\n",
    "    Args:\n",
    "        url (str): The Wikipedia URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (education, office_date).\n",
    "               Returns specific error messages or defaults if info not found.\n",
    "    \"\"\"\n",
    "    # Handle potential non-string or NaN URLs\n",
    "    if not isinstance(url, str) or pd.isna(url) or not url.startswith('http'):\n",
    "        return \"Invalid or Missing URL\", \"Invalid or Missing URL\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'MyCoolScraper/1.0 (contact@example.com)'} # Be polite, identify your bot\n",
    "        response = requests.get(url, headers=headers, timeout=10) # Add timeout\n",
    "        response.raise_for_status() # Checks for HTTP errors (4xx or 5xx)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        infobox = soup.find('table', class_='infobox') # More general infobox class\n",
    "\n",
    "        # Try another common infobox class if the first fails\n",
    "        if not infobox:\n",
    "                infobox = soup.find('table', class_='infobox vcard')\n",
    "\n",
    "        if not infobox:\n",
    "            return \"Infobox Not Found\", \"Infobox Not Found\"\n",
    "\n",
    "        education = \"Education Not Found in Infobox\"\n",
    "        office_date = \"Office Date Not Found in Infobox\" # Default for office date\n",
    "\n",
    "        # Search for relevant information within the infobox rows\n",
    "        rows = infobox.find_all('tr')\n",
    "        for i, row in enumerate(rows):\n",
    "            header = row.find('th')\n",
    "            data_cell = row.find('td')\n",
    "\n",
    "            if header:\n",
    "                header_text = header.text.strip().lower()\n",
    "\n",
    "                # Find Education or Alma Mater\n",
    "                if 'education' in header_text or 'alma mater' in header_text:\n",
    "                    if data_cell:\n",
    "                        # Improve extraction: get text, handle line breaks, strip whitespace\n",
    "                        education = data_cell.get_text(separator='; ', strip=True)\n",
    "                        # Don't break yet, continue searching for office date\n",
    "\n",
    "                # Find Assumed Office or In Office (prioritize Assumed Office if both exist)\n",
    "                if 'assumed office' in header_text:\n",
    "                   # The date is often in the next row's td if the header spans the full row,\n",
    "                   # or in the same row's td if it's a label-data pair.\n",
    "                   # Check if the header has colspan=\"2\" or if there's no td in the current row\n",
    "                   if header.get('colspan') == '2' or not data_cell:\n",
    "                       # Look in the next row's td\n",
    "                       if i + 1 < len(rows):\n",
    "                           next_row_data = rows[i+1].find('td')\n",
    "                           if next_row_data:\n",
    "                               # Check if the next row cell is not intended for something else (like predecessor/successor)\n",
    "                               prev_th = rows[i+1].find('th')\n",
    "                               if not prev_th or not prev_th.text.strip(): # Ensure it's not a labeled row\n",
    "                                    office_date = next_row_data.get_text(separator=' ', strip=True)\n",
    "                                    # Prefer 'Assumed office' if found, potentially overwrite 'In office'\n",
    "                   elif data_cell: # Standard th/td row structure\n",
    "                       office_date = data_cell.get_text(separator=' ', strip=True)\n",
    "\n",
    "                elif 'in office' in header_text and office_date == \"Office Date Not Found in Infobox\":\n",
    "                    # Similar logic as 'Assumed office', but only if 'Assumed office' wasn't found\n",
    "                   if header.get('colspan') == '2' or not data_cell:\n",
    "                       if i + 1 < len(rows):\n",
    "                           next_row_data = rows[i+1].find('td')\n",
    "                           if next_row_data:\n",
    "                               prev_th = rows[i+1].find('th')\n",
    "                               if not prev_th or not prev_th.text.strip():\n",
    "                                   office_date = next_row_data.get_text(separator=' ', strip=True)\n",
    "                   elif data_cell:\n",
    "                       office_date = data_cell.get_text(separator=' ', strip=True)\n",
    "\n",
    "            # Sometimes the \"Assumed office\" text is not in a `th` but directly above the date `td`\n",
    "            # Example: <tr><td colspan=\"2\" style=\"border-bottom:none\"><span class=\"nowrap\"><b>Assumed office</b></span> <br>January 3, 2021</td></tr>\n",
    "            elif data_cell and data_cell.get('colspan') == '2':\n",
    "                 bold_span = data_cell.find('span', class_='nowrap')\n",
    "                 if bold_span and bold_span.find('b'):\n",
    "                     bold_text = bold_span.find('b').text.strip().lower()\n",
    "                     if bold_text == 'assumed office':\n",
    "                         # Extract text following the <br> tag if present, otherwise the whole cell text after the span\n",
    "                         br = data_cell.find('br')\n",
    "                         if br and br.next_sibling:\n",
    "                             office_date = br.next_sibling.strip()\n",
    "                         else: # Fallback if no <br> or text after it\n",
    "                             office_date = data_cell.get_text(separator=' ', strip=True).replace(bold_span.text.strip(),'').strip()\n",
    "\n",
    "                     elif bold_text == 'in office' and office_date == \"Office Date Not Found in Infobox\":\n",
    "                         br = data_cell.find('br')\n",
    "                         if br and br.next_sibling:\n",
    "                              office_date = br.next_sibling.strip()\n",
    "                         else:\n",
    "                             office_date = data_cell.get_text(separator=' ', strip=True).replace(bold_span.text.strip(),'').strip()\n",
    "\n",
    "        # Clean up potential \"[...] years ago)\" text from dates like 'Incumbent' might add\n",
    "        if '(' in office_date and 'age' not in office_date.lower(): # Avoid removing age parens\n",
    "             office_date = office_date.split('(')[0].strip()\n",
    "\n",
    "        return education, office_date\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request Error: {e}\", f\"Request Error: {e}\" # Specific error for network issues\n",
    "    except Exception as e:\n",
    "        # Log the URL that caused the parsing error for debugging\n",
    "        print(f\"Parsing Error for URL: {url} - Error: {e}\")\n",
    "        return f\"Parsing Error: {e}\", f\"Parsing Error: {e}\" # Specific error for parsing issues\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"3bio-checked-n-uni-checked-senate.csv\"  # Update with your actual file name\n",
    "try:\n",
    "    # Try common encodings if ISO-8859-1 fails\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding=\"UTF-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file, encoding=\"ISO-8859-1\") # Try UTF-16 as a common alternative\n",
    "\n",
    "except FileNotFoundError:\n",
    "    raise Exception(f\"Error: CSV file '{csv_file}' not found. Cannot proceed without the CSV file.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error reading CSV file '{csv_file}': {e}. Cannot proceed.\")\n",
    "\n",
    "# Ensure the URL column exists\n",
    "if 'wikipedia_url' not in df.columns:\n",
    "    print(f\"Error: Column 'wikipedia_url' not found in '{csv_file}'.\")\n",
    "    exit() # Exit if the column is missing\n",
    "\n",
    "# Apply the function and store results\n",
    "education_results = []\n",
    "office_date_results = []\n",
    "total_rows = len(df['wikipedia_url'])\n",
    "\n",
    "print(f\"Starting Wikipedia scrape for {total_rows} URLs...\")\n",
    "\n",
    "for index, url in enumerate(df['wikipedia_url']):\n",
    "    print(f\"Processing URL {index + 1}/{total_rows}: {url}\") # Progress indicator\n",
    "    edu_info, office_info = get_info_from_wikipedia(url)\n",
    "    education_results.append(edu_info)\n",
    "    office_date_results.append(office_info)\n",
    "    time.sleep(0.5) # Wait half a second between requests - BE POLITE!\n",
    "\n",
    "print(\"Scraping complete. Adding results to DataFrame...\")\n",
    "\n",
    "# Add the results as new columns to the DataFrame\n",
    "df['education'] = education_results\n",
    "df['office_start_date'] = office_date_results # New column for the date\n",
    "\n",
    "# Save the results to a new CSV\n",
    "output_csv = \"4aboutpage-n-wiki-vcard.csv\" # Use a new output name\n",
    "df.to_csv(output_csv, index=False, encoding='UTF-8') # Save as UTF-16 for better compatibility\n",
    "\n",
    "print(f\"Extraction complete. Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean wikipedia vcard university into individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and splitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('4aboutpage-n-wiki-vcard.csv')\n",
    "\n",
    "def clean_and_split_education(education_str):\n",
    "    if not isinstance(education_str, str):\n",
    "        return []  # Return an empty list when the input is not a string\n",
    "    cleaned_str = re.sub(r'\\([^)]*\\)', '', education_str)\n",
    "    universities = [uni.strip() for uni in cleaned_str.split(';') if uni.strip()]\n",
    "    return universities\n",
    "\n",
    "df['education'] = df['education'].apply(clean_and_split_education)\n",
    "\n",
    "max_universities = df['education'].apply(len).max()\n",
    "\n",
    "for i in range(max_universities):\n",
    "    df[f'University_{i+1}'] = df['education'].apply(lambda x: x[i] if i < len(x) else '')\n",
    "\n",
    "df.drop(columns=['education'], inplace=True)\n",
    "\n",
    "df.to_csv('5aboutpage-n-cleaned-uni.csv', index=False)\n",
    "\n",
    "print(\"Cleaning and splitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape about page bio\n",
    "- First following block contains openAI code and subsequent block contains deepseek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://barrymoore.house.gov/about\n",
      "OpenAI API error for https://barrymoore.house.gov/about: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Processing URL: https://figures.house.gov/about\n",
      "OpenAI API error for https://figures.house.gov/about: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Processing URL: https://mikerogers.house.gov/about/\n",
      "OpenAI API error for https://mikerogers.house.gov/about/: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Scraping completed. Results saved to scraped-bio-n-cleaned-uni.csv\n"
     ]
    }
   ],
   "source": [
    "# OpenAI code\n",
    "client = openai.OpenAI()  # Create the client\n",
    "\n",
    "def scrape_bio(url):\n",
    "    if not url or url.lower() == \"not found\":\n",
    "        print(f\"Skipping invalid URL: {url}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page_text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    chunks = [page_text[i:i + 2000] for i in range(0, len(page_text), 2000)]\n",
    "    full_bio = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"Extract the following text exactly as it is without summarizing:\\n\\n{chunk}\"\n",
    "        try:\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a text extraction tool.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1500,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            full_bio += chat_response.choices[0].message.content.strip() + \"\\n\"\n",
    "        except openai.OpenAIError as e:\n",
    "            print(f\"OpenAI API error for {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    return full_bio.strip()\n",
    "\n",
    "# File paths\n",
    "input_file = \"5aboutpage-n-cleaned-uni.csv\"\n",
    "output_file = \"6scraped-bio-n-cleaned-uni.csv\"\n",
    "\n",
    "# Read input CSV and process each URL\n",
    "with open(input_file, 'r', encoding='UTF-16') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    fieldnames = reader.fieldnames + ['bio']\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='UTF-16') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            url = row.get('candidates official page URL', '').strip()\n",
    "            print(f\"Processing URL: {url}\")\n",
    "\n",
    "            if url:\n",
    "                row['bio'] = scrape_bio(url)\n",
    "            else:\n",
    "                row['bio'] = \"\"\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"Scraping completed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\rosh\\desktop\\all\\columbia\\classes\\y4s1\\cubs\\cubs-elitism-signal\\v1-2\\bio-scrape\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing URL: https://barrymoore.house.gov/about\n",
      "API request failed: 401 Client Error: Unauthorized for url: https://api.deepseek.com/chat/completions\n",
      "Failed to get valid response for https://barrymoore.house.gov/about\n",
      "Processing URL: https://figures.house.gov/about\n",
      "API request failed: 401 Client Error: Unauthorized for url: https://api.deepseek.com/chat/completions\n",
      "Failed to get valid response for https://figures.house.gov/about\n",
      "Processing URL: https://mikerogers.house.gov/about/\n",
      "API request failed: 401 Client Error: Unauthorized for url: https://api.deepseek.com/chat/completions\n",
      "Failed to get valid response for https://mikerogers.house.gov/about/\n",
      "Scraping completed. Results saved to scraped-bio-n-cleaned-uni.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import time # Added for potential rate limiting\n",
    "\n",
    "# --- Environment Variable Loading ---\n",
    "# Load environment variables from .env file located in the script's directory or parent directories\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key using its variable NAME defined in the .env file\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# CRITICAL: Check if the API key was loaded successfully\n",
    "if not DEEPSEEK_API_KEY:\n",
    "    raise ValueError(\"DeepSeek API key not found in environment variables. \"\n",
    "                     \"Ensure you have a .env file in the script's directory \"\n",
    "                     \"with the line: DEEPSEEK_API_KEY=your_actual_key\")\n",
    "\n",
    "# --- DeepSeek API Interaction ---\n",
    "def query_deepseek(prompt):\n",
    "    \"\"\"Send a prompt to DeepSeek's API and return the response.\"\"\"\n",
    "    headers = {\n",
    "        # Use the correctly loaded API key here\n",
    "        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"deepseek-chat\",  # Verify this is the correct/desired model\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a text extraction tool. Extract the following text exactly as it is, without summarizing or altering it.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 8192, # Increased slightly, adjust as needed based on chunk size and typical response\n",
    "        \"temperature\": 0.0 # Keep temperature low for exact extraction\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.deepseek.com/chat/completions\", # Verify the correct API endpoint\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            timeout=30 # Added a longer timeout for potentially slow API responses\n",
    "        )\n",
    "        # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Print detailed error, including status code if available\n",
    "        status_code = e.response.status_code if e.response is not None else \"N/A\"\n",
    "        print(f\"API request failed: {str(e)} (Status Code: {status_code})\")\n",
    "        # Optional: Add a small delay before retrying or failing\n",
    "        # time.sleep(1)\n",
    "        return None\n",
    "\n",
    "# --- Web Scraping Function ---\n",
    "def scrape_bio(url):\n",
    "    \"\"\"Scrapes text from a URL and uses DeepSeek API for extraction.\"\"\"\n",
    "    if not url or url.lower() == \"not found\":\n",
    "        print(f\"Skipping invalid or missing URL: {url}\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        # Added headers to potentially mimic a browser\n",
    "        scrape_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, timeout=15, headers=scrape_headers) # Increased timeout\n",
    "        response.raise_for_status() # Check for HTTP errors like 404 Not Found\n",
    "        response.encoding = response.apparent_encoding # Try to handle encoding better\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return \"\" # Return empty string on fetch error\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Try to find main content area if possible (improves quality, requires site inspection)\n",
    "    # Example: main_content = soup.find('main') or soup.find('article') or soup.find(id='content')\n",
    "    # if main_content:\n",
    "    #     page_text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "    # else:\n",
    "    #     page_text = soup.get_text(separator=\"\\n\", strip=True) # Fallback to full text\n",
    "\n",
    "    page_text = soup.get_text(separator=\"\\n\", strip=True) # Using full text as per original code\n",
    "\n",
    "    if not page_text:\n",
    "        print(f\"No text extracted from {url}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Define chunk size - ensure it's less than API token limits considering prompt+response\n",
    "    chunk_size = 1800 # Characters per chunk (adjust based on testing and token limits)\n",
    "    chunks = [page_text[i:i + chunk_size] for i in range(0, len(page_text), chunk_size)]\n",
    "    full_bio = \"\"\n",
    "    print(f\"Splitting text from {url} into {len(chunks)} chunks.\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        # The prompt asks the model to just return the text provided\n",
    "        prompt = f\"Extract the following text exactly as it is without summarizing or adding anything:\\n\\n---\\n{chunk}\\n---\"\n",
    "        api_response = query_deepseek(prompt)\n",
    "\n",
    "        if api_response and \"choices\" in api_response and len(api_response[\"choices\"]) > 0:\n",
    "            extracted_text = api_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            full_bio += extracted_text + \"\\n\" # Append extracted text\n",
    "            # Optional: Add a small delay between API calls to avoid rate limits\n",
    "            # time.sleep(0.5)\n",
    "        else:\n",
    "            print(f\"Failed to get valid response for chunk {i+1} from {url}\")\n",
    "            # Decide how to handle partial failure: continue, return partial, or return \"\"\n",
    "            # Returning \"\" on first failure to ensure data integrity, modify if partial results are okay\n",
    "            return \"\" # Failed to process a chunk, abort for this URL\n",
    "\n",
    "    print(f\"Successfully processed {url}\")\n",
    "    return full_bio.strip()\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "# Define input and output file paths\n",
    "input_file = \"5aboutpage-n-cleaned-uni.csv\"\n",
    "output_file = \"6scraped-bio-n-cleaned-uni.csv\"\n",
    "\n",
    "try:\n",
    "    # Read input CSV and process each URL\n",
    "    with open(input_file, 'r', encoding='UTF-16') as csvfile:\n",
    "        # Check if file is empty\n",
    "        first_char = csvfile.read(1)\n",
    "        if not first_char:\n",
    "            print(f\"Error: Input file '{input_file}' is empty.\")\n",
    "            exit()\n",
    "        csvfile.seek(0) # Reset file pointer\n",
    "\n",
    "        reader = csv.DictReader(csvfile)\n",
    "\n",
    "        # Ensure the 'candidates official page URL' column exists\n",
    "        if 'candidates official page URL' not in reader.fieldnames:\n",
    "             raise ValueError(f\"Column 'candidates official page URL' not found in '{input_file}'. \"\n",
    "                              f\"Available columns: {reader.fieldnames}\")\n",
    "\n",
    "        # Prepare fieldnames for the output file\n",
    "        fieldnames = reader.fieldnames + ['bio']\n",
    "\n",
    "        # Write to output CSV\n",
    "        with open(output_file, 'w', newline='', encoding='UTF-16') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                # Get the URL, default to empty string if column missing or empty\n",
    "                url = row.get('candidates official page URL', '').strip()\n",
    "\n",
    "                # Scrape the biography using the function\n",
    "                row['bio'] = scrape_bio(url) # scrape_bio handles empty/invalid URLs\n",
    "\n",
    "                # Write the original row data plus the new bio\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Scraping completed. Results saved to {output_file}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{input_file}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if uni is prestige & if bio mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite status and combined uni mention checks added.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('6scraped-bio-n-cleaned-uni.csv', encoding='latin1')\n",
    "elite_df = pd.read_csv('T25-list.csv')\n",
    "elite_list = elite_df['University'].tolist()\n",
    "\n",
    "def is_elite(university):\n",
    "    return 1 if university in elite_list else 0\n",
    "\n",
    "# Add elite status for each university column\n",
    "for i in range(1, 6):\n",
    "    df[f'University_{i}_Elite'] = df[f'University_{i}'].apply(is_elite)\n",
    "\n",
    "def combined_uni_in_bio(row):\n",
    "    # Extract university mentions only once\n",
    "    bio_text = row['bio']\n",
    "    if not isinstance(bio_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    keywords = [\"college\", \"school\", \"university\", \"academy\", \"degree\", \"masters\", \"bachelor\", \"associate\", \"phd\", \"doctorate\", \"GED\", \"MBA\", \"cum laude\", \"attended\", \"graduated\", \"studied\", \"enrolled\", \"graduate\", \"alumni\", \"alma mater\"]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\n+', bio_text)\n",
    "    matches = [\n",
    "        sentence.strip()\n",
    "        for sentence in sentences\n",
    "        if any(keyword in sentence.lower() for keyword in keywords)\n",
    "           and \"military academy nominations\" not in sentence.lower()\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\".join(matches) if matches else \"\"\n",
    "\n",
    "# Create one combined column with all university-related sentences\n",
    "df['uni_in_bio'] = df.apply(combined_uni_in_bio, axis=1)\n",
    "\n",
    "df.to_csv('7bio-checked-n-uni-checked.csv', index=False)\n",
    "print(\"Elite status and combined uni mention checks added.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for university aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matching completed. Results saved to '8final_university_matching.csv'\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Fix encoding issues\n",
    "    text = text.replace('', \"'\")\n",
    "    text = text.replace('', \"'\")\n",
    "    # Remove punctuation and make lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "    return ' '.join(text.split())  # Remove extra whitespace\n",
    "\n",
    "def is_university_in_bio(university, bio):\n",
    "    if pd.isna(university) or pd.isna(bio):\n",
    "        return 0\n",
    "    \n",
    "    clean_uni = clean_text(university)\n",
    "    clean_bio = clean_text(bio)\n",
    "    \n",
    "    # Dictionary of university variations (key = main name, value = list of aliases)\n",
    "    university_variations = {\n",
    "        'adelphi university': ['adelphi'],\n",
    "        'air university': ['au'],\n",
    "        'albright college': ['albright'],\n",
    "        'alliant international university': ['alliant'],\n",
    "        'alma college': ['alma'],\n",
    "        'american university': ['au'],\n",
    "        'amherst college': ['amherst'],\n",
    "        'appalachian state university': ['app state', 'asu'],\n",
    "        'arizona state university tempe': ['arizona state university', 'asu', 'arizona state'],\n",
    "        'arizona state university phoenix': ['asu phoenix', 'arizona state phoenix'],\n",
    "        'auburn university': ['auburn'],\n",
    "        'ball state university': ['ball state'],\n",
    "        'baylor university': ['baylor'],\n",
    "        'biola university': ['biola'],\n",
    "        'binghamton university': ['suny binghamton', 'binghamton'],\n",
    "        'birmingham southern college': ['birminghamsouthern college', 'bsc'],\n",
    "        'boston college': ['bc'],\n",
    "        'boston university': ['bu'],\n",
    "        'brigham young university': ['byu'],\n",
    "        'brooklyn college': ['brooklyn'],\n",
    "        'brown university': ['brown'],\n",
    "        'butler community college': ['butler cc'],\n",
    "        'california institute of technology': ['caltech'],\n",
    "        'california polytechnic state university san luis obispo': ['cal poly'],\n",
    "        'capella university': ['capella'],\n",
    "        'carnegie mellon university': ['cmu'],\n",
    "        'case western reserve university': ['case western', 'cwru'],\n",
    "        'charleston southern university': ['charleston southern'],\n",
    "        'chattanooga state community college': ['chattanooga state', 'chattanooga cc'],\n",
    "        'clark county community college': ['clark county cc'],\n",
    "        'college of the holy cross': ['holy cross'],\n",
    "        'columbia university': ['columbia'],\n",
    "        'columbus state university': ['columbus state'],\n",
    "        'concordia college': ['concordia'],\n",
    "        'copiaha lincoln community college': ['copiaha lincoln cc'],\n",
    "        'cornell university': ['cornell'],\n",
    "        'dallas theological seminary': ['dallas seminary'],\n",
    "        'dartmouth college': ['dartmouth'],\n",
    "        'deep springs college': ['deep springs'],\n",
    "        'duke university': ['duke'],\n",
    "        'embry-riddle aeronautical university': ['erau', 'embry riddle'],\n",
    "        'emory university': ['emory'],\n",
    "        'fairleigh dickinson university': ['fdu', 'fairleigh dickinson'],\n",
    "        'florida state university': ['fsu'],\n",
    "        'fort hays state university': ['fhsu', 'fort hays state'],\n",
    "        'george mason university': ['gmu'],\n",
    "        'george washington university': ['gwu', 'gw'],\n",
    "        'georgetown university': ['georgetown'],\n",
    "        'georgia institute of technology': ['georgia tech', 'gatech'],\n",
    "        'gonzaga university': ['gonzaga'],\n",
    "        'grace college and seminary': ['grace college'],\n",
    "        'harvard university': ['harvard law school', 'harvard college', 'harvard'],\n",
    "        'indiana university bloomington': ['indiana university', 'iu bloomington', 'iu'],\n",
    "        'indiana university indianapolis': ['iupui', 'iu indianapolis'],\n",
    "        'iowa state university': ['isu', 'iowa state'],\n",
    "        'johns hopkins university': ['jhu', 'johns hopkins'],\n",
    "        'kansas state university': ['k-state', 'ksu'],\n",
    "        'kent state university': ['kent state'],\n",
    "        'london school of economics': ['lse'],\n",
    "        'louisiana state university': ['lsu'],\n",
    "        'magdalen college oxford': ['magdalen college'],\n",
    "        'marshall university': ['marshall'],\n",
    "        'massachusetts institute of technology': ['mit'],\n",
    "        'miami university': ['miami ohio', 'miami u'],\n",
    "        'michigan state university': ['msu'],\n",
    "        'mississippi state university': ['mississippi state', 'msu'],\n",
    "        'missouri valley college': ['missouri valley'],\n",
    "        'montana state university': ['montana state', 'msu'],\n",
    "        'morehouse college': ['morehouse'],\n",
    "        'naval postgraduate school': ['nps'],\n",
    "        'new mexico highlands university': ['nmhu'],\n",
    "        'new york university': ['nyu'],\n",
    "        'north carolina state university': ['nc state', 'ncsu'],\n",
    "        'northeastern university': ['northeastern'],\n",
    "        'northern illinois university': ['niu'],\n",
    "        'northwestern university': ['northwestern'],\n",
    "        'ohio state university': ['osu', 'ohio state'],\n",
    "        'oklahoma state university institute of technology': ['osuit'],\n",
    "        'pennsylvania state university': ['penn state', 'psu'],\n",
    "        'pennsylvania state university university park': ['penn state main campus'],\n",
    "        'pomona college': ['pomona'],\n",
    "        'presbyterian college': ['presbyterian'],\n",
    "        'princeton university': ['princeton'],\n",
    "        'purdue university': ['purdue'],\n",
    "        'queens college oxford': [\"queen's college, oxford\", \"queen's college\"],\n",
    "        'rutgers university': ['rutgers'],\n",
    "        'rutgers university new brunswick': ['rutgers main campus'],\n",
    "        'saint louis university': ['slu'],\n",
    "        'samford university': ['cumberland school of law', 'samford'],\n",
    "        'san diego state university': ['sdsu'],\n",
    "        'shippensburg university': ['shippensburg'],\n",
    "        'smith college': ['smith'],\n",
    "        'south dakota state university': ['sdsu'],\n",
    "        'southern college of optometry': ['sco'],\n",
    "        'southern methodist university': ['smu'],\n",
    "        'southern state college': ['southern state'],\n",
    "        'southwestern baptist theological seminary': ['swbts'],\n",
    "        'st lawrence university': ['saint lawrence university', 'st lawrence'],\n",
    "        'st marys university texas': [\"st. mary's university, texas\", \"saint mary's university\"],\n",
    "        'stanford university': ['stanford'],\n",
    "        'stetson university': ['stetson'],\n",
    "        'stony brook university': ['suny stony brook'],\n",
    "        'swarthmore college': ['swarthmore'],\n",
    "        'texas a&m university': ['tamu', 'texas am'],\n",
    "        'texas tech university': ['ttu', 'texas tech'],\n",
    "        'trinity college cambridge': ['trinity college'],\n",
    "        'trinity university': ['trinity'],\n",
    "        'truman state university': ['truman state'],\n",
    "        'tufts university': ['tufts'],\n",
    "        'tulane university': ['tulane'],\n",
    "        'union theological seminary': ['union seminary'],\n",
    "        'united states merchant marine academy': ['usmma', 'merchant marine academy'],\n",
    "        'united states military academy': ['west point', 'usma'],\n",
    "        'united states naval academy': ['annapolis', 'usna'],\n",
    "        'university of alabama': ['ua', 'alabama university', 'bama'],\n",
    "        'university of arizona': ['uofa', 'u of a'],\n",
    "        'university of arkansas': ['uark', 'arkansas'],\n",
    "        'university of california berkeley': ['uc berkeley', 'cal', 'ucb', 'university of california, berkeley'],\n",
    "        'university of california los angeles': ['ucla', 'university of california, los angeles'],\n",
    "        'university of california santa barbara': ['ucsb', 'university of california, santa barbara'],\n",
    "        'university of chicago': ['uchicago'],\n",
    "        'university of colorado boulder': ['cu boulder', 'colorado'],\n",
    "        'university of connecticut': ['uconn'],\n",
    "        'university of dayton': ['ud', 'dayton'],\n",
    "        'university of delaware': ['udel', 'delaware'],\n",
    "        'university of detroit': ['detroit mercy', 'udm'],\n",
    "        'university of florida': ['uf', 'florida'],\n",
    "        'university of georgia': ['uga'],\n",
    "        'university of guam': ['guam university', 'uog'],\n",
    "        'university of hawaii': ['uh', 'hawaii'],\n",
    "        'university of houston': ['uh'],\n",
    "        'university of idaho': ['uidaho'],\n",
    "        'university of illinois at urbana-champaign': ['uiuc', 'illinois'],\n",
    "        'university of iowa': ['uiowa', 'iowa'],\n",
    "        'university of kansas': ['ku', 'kansas'],\n",
    "        'university of kentucky': ['uk', 'kentucky'],\n",
    "        'university of london': ['uol'],\n",
    "        'university of louisville': ['uofl', 'louisville'],\n",
    "        'university of maryland baltimore': ['umd baltimore', 'university of maryland, baltimore'],\n",
    "        'university of maryland college park': ['umd', 'maryland'],\n",
    "        'university of maryland university college': ['umuc', 'umgc'],\n",
    "        'university of mary': ['umary'],\n",
    "        'university of michigan': ['umich', 'michigan'],\n",
    "        'university of minnesota': ['u of m', 'umn'],\n",
    "        'university of mississippi': ['ole miss'],\n",
    "        'university of missouri': ['mizzou', 'mu'],\n",
    "        'university of missouri kansas city': ['umkc', 'university of missourikansas city'],\n",
    "        'university of nebraska lincoln': ['unl', 'university of nebraska, lincoln'],\n",
    "        'university of nevada reno': ['unr', 'university of nevada, reno'],\n",
    "        'university of new mexico': ['unm'],\n",
    "        'university of north carolina at chapel hill': ['unc', 'north carolina'],\n",
    "        'university of northern iowa': ['uni'],\n",
    "        'university of notre dame': ['notre dame'],\n",
    "        'university of oklahoma': ['ou', 'oklahoma'],\n",
    "        'university of oregon': ['uoregon', 'uo'],\n",
    "        'university of pennsylvania': ['upenn', 'penn'],\n",
    "        'university of south carolina': ['usc', 'south carolina'],\n",
    "        'university of south dakota': ['usd'],\n",
    "        'university of southern california': ['usc'],\n",
    "        'university of southern mississippi': ['southern miss', 'usm'],\n",
    "        'university of tennessee': ['ut', 'utk'],\n",
    "        'university of texas at austin': ['ut austin', 'utexas', 'ut'],\n",
    "        'university of virginia': ['uva'],\n",
    "        'university of washington': ['uw', 'u dub'],\n",
    "        'university of wisconsin milwaukee': ['uw', 'uwm', 'university of wisconsin, milwaukee'],\n",
    "        'university of wyoming': ['uwyo'],\n",
    "        'vanderbilt university': ['vanderbilt', 'vandy'],\n",
    "        'villanova university': ['villanova', 'nova'],\n",
    "        'virginia tech': ['virginia polytechnic institute', 'vt'],\n",
    "        'wake forest university': ['wake', 'wake forest'],\n",
    "        'washington state university': ['wsu', 'wash state'],\n",
    "        'washington university': ['washu'],\n",
    "        'wayne state university': ['wsu', 'wayne state'],\n",
    "        'wesleyan university': ['wesleyan'],\n",
    "        'willamette university': ['willamette'],\n",
    "        'williams college': ['williams'],\n",
    "        'yale university': ['yale'],\n",
    "        'oxford university': ['university of oxford', 'oxford', 'pembroke college', 'merton college', 'st hildas college oxford'],\n",
    "        'cambridge university': ['university of cambridge', 'cambridge', 'king\\'s college', 'st johns college', 'saint johns college cambridge']\n",
    "    }\n",
    "    \n",
    "    # Check direct match first\n",
    "    if clean_uni in clean_bio:\n",
    "        return 1\n",
    "    \n",
    "    # Check if any variation matches\n",
    "    for main_name, aliases in university_variations.items():\n",
    "        if clean_uni in main_name:  # Check if our university is a main name\n",
    "            for alias in aliases + [main_name]:\n",
    "                if alias in clean_bio:\n",
    "                    return 1\n",
    "        elif any(alias in clean_uni for alias in aliases):  # Check if our university is an alias\n",
    "            if main_name in clean_bio:\n",
    "                return 1\n",
    "    \n",
    "    # Special handling for law/medical schools\n",
    "    if 'law school' in clean_bio and ('law' in clean_uni or 'university' in clean_uni):\n",
    "        return 1\n",
    "    \n",
    "    # Try fuzzy matching as last resort\n",
    "    if fuzz.partial_ratio(clean_uni, clean_bio) >= 85:\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('7bio-checked-n-uni-checked.csv', encoding='latin1')\n",
    "\n",
    "# Create new columns for each university column, returning 1 for True and 0 for False\n",
    "for i in range(1, 6):\n",
    "    col_name = f'University_{i}'\n",
    "    new_col_name = f'University_{i}_in_bio'\n",
    "    df[new_col_name] = df.apply(lambda row: is_university_in_bio(row[col_name], row['uni_in_bio']), axis=1)\n",
    "\n",
    "# Save the results\n",
    "df.to_csv('8final_university_matching.csv', index=False, encoding='utf-8')\n",
    "print(\"Final matching completed. Results saved to '8final_university_matching.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM to vet uni mention in bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University verification complete. Results saved to 9LLM-vetted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('8final_university_matchingsenate.csv', encoding='Latin1')\n",
    "\n",
    "# Initialize DeepSeek API parameters\n",
    "DEEPSEEK_API_KEY = 'DEEPSEEK-API-KEY'  # Replace with your actual API key\n",
    "DEEPSEEK_API_URL = 'https://api.deepseek.com/v1/chat/completions'  # Example endpoint, adjust as needed\n",
    "\n",
    "def check_university_in_bio(university, bio_text):\n",
    "    \"\"\"Use DeepSeek API to check if university is mentioned in bio\"\"\"\n",
    "    if pd.isna(university) or pd.isna(bio_text):\n",
    "        return 0\n",
    "    \n",
    "    prompt = f\"Does the following text mention '{university}'? Answer only '1' for yes or '0' for no. Text: {bio_text}\"\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {DEEPSEEK_API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        'model': 'deepseek-chat',  # Adjust model as needed\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        answer = result['choices'][0]['message']['content'].strip()\n",
    "        return int(answer) if answer in ['0', '1'] else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking university {university}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Create new columns for DeepSeek verification\n",
    "for i in range(1, 6):\n",
    "    col_name = f'University_{i}'\n",
    "    new_col_name = f'DeepSeek_vetted_uni{i}'\n",
    "    \n",
    "    df[new_col_name] = df.apply(\n",
    "        lambda row: check_university_in_bio(row[col_name], row['uni_in_bio']),\n",
    "        axis=1\n",
    "    )\n",
    "# Save the updated dataframe\n",
    "df.to_csv('9LLM-vetted.csv', index=False)\n",
    "\n",
    "# Add column based on comparing the university_in_bio and DeepSeek_vetted values\n",
    "df[\"LLM_vetted_result\"] = df.apply(\n",
    "    lambda row: 0 if (\n",
    "        row[\"University_1_in_bio\"] == row[\"DeepSeek_vetted_uni1\"] and\n",
    "        row[\"University_2_in_bio\"] == row[\"DeepSeek_vetted_uni2\"] and\n",
    "        row[\"University_3_in_bio\"] == row[\"DeepSeek_vetted_uni3\"] and\n",
    "        row[\"University_4_in_bio\"] == row[\"DeepSeek_vetted_uni4\"] and\n",
    "        row[\"University_5_in_bio\"] == row[\"DeepSeek_vetted_uni5\"]\n",
    "    ) else 1,\n",
    "    axis=1\n",
    ")\n",
    "# Save the updated dataframe\n",
    "df.to_csv('9LLM-vetted.csv', index=False)\n",
    "\n",
    "print(\"University verification complete. Results saved to 9LLM-vetted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioscrape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
